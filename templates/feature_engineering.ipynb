{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-nepal",
   "metadata": {},
   "source": [
    "## Feature Engineering Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-skiing",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "weekly-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import math\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    CondensedNearestNeighbour,\n",
    "    TomekLinks,\n",
    "    OneSidedSelection,\n",
    "    EditedNearestNeighbours,\n",
    "    RepeatedEditedNearestNeighbours,\n",
    "    AllKNN,\n",
    "    NeighbourhoodCleaningRule,\n",
    "    NearMiss,\n",
    "    InstanceHardnessThreshold\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    "    SMOTE,\n",
    "    ADASYN,\n",
    "    BorderlineSMOTE,\n",
    "    SVMSMOTE,\n",
    ")\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "from imblearn.ensemble import (\n",
    "    BalancedBaggingClassifier,\n",
    "    BalancedRandomForestClassifier,\n",
    "    RUSBoostClassifier,\n",
    "    EasyEnsembleClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "\n",
    "\n",
    "# adding common folder location to sys.path\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "\n",
    "from helper import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-salad",
   "metadata": {},
   "source": [
    "### Loading Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-investment",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your dataset, can be a csv file or xlsx\n",
    "dataset_path = \"../Bank_Personal_Loan_Modelling.xlsx\"\n",
    "\n",
    "## use code as per the type of data source\n",
    "\n",
    "## use below line to read data from csv file\n",
    "## df = pd.read_csv(dataset_path)\n",
    "df = pd.read_excel(dataset_path, sheet_name = 1, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Personal Loan'\n",
    "# df_x = df.drop(columns=[target])\n",
    "# df_y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-relaxation",
   "metadata": {},
   "source": [
    "Let's separate out, numerical, categorical and numerical_normal and numerical_non_normal attributes<br/>\n",
    "This list will be used for outliers treatment and other transformation later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-vermont",
   "metadata": {},
   "source": [
    "### Separating Numerical and Categorical attributes along with normal and non normal numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts method can be used to see if an attribute contains categorical data or continous data\n",
    "unique_val_in_cols = df.apply( lambda col : col.nunique()).sort_values()\n",
    "print(unique_val_in_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide reasnable threshold value for separating categorical and numerical attributes based on above result\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes = list(unique_val_in_cols[unique_val_in_cols < threshold].keys())\n",
    "numerical_attributes = list(unique_val_in_cols[unique_val_in_cols > threshold].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_test(df, significance = .01):\n",
    "    \"\"\"\n",
    "    Function to perform ks test and test against normal distribution using  \n",
    "    D’Agostino, R. B. (1971), “An omnibus test of normality for moderate and large sample size”\n",
    "    \n",
    "    frame: a pandas dataframe\n",
    "    significance: float. Alpha level for which the null hypotesis will be rejected (H0: series comes from a normal distribution)\n",
    "    plot: Boolean, whether or not plot a histogram for resulting columns\n",
    "    \n",
    "    returns a dataframe with only those columns that follow a normal distribution according to test.\n",
    "\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    non_normal_columns = []\n",
    "\n",
    "    for col in columns:\n",
    "        aux = df[col]\n",
    "    \n",
    "        _, p = normaltest(aux)\n",
    "    \n",
    "        if p <= significance:\n",
    "            # col is not normally distributed\n",
    "            non_normal_columns.append(col)\n",
    "        \n",
    "    normal_columns = [cols for cols in columns if cols not in non_normal_columns]\n",
    "    return normal_columns, non_normal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_attributes, non_normal_attributes = normal_test(df[numerical_attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of attributes:\")\n",
    "print(\"categorical:{0}\".format(len(categorical_attributes)))\n",
    "print(\"numerical:{0}\".format(len(numerical_attributes)))\n",
    "print(\"normal attributes:{0}\".format(len(normal_attributes)))\n",
    "print(\"non normal attributes:{0}\".format(len(non_normal_attributes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-category",
   "metadata": {},
   "source": [
    "#### Removing Target column from the categorical list of variables so that it does not get transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes.remove(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-pierre",
   "metadata": {},
   "source": [
    "### Perform basic data cleaning as per observations from EDA\n",
    "* For this dataset, we know that the 'Experience' column has minor percentage of negative values(which is wrong) so let's treat it before using any other transformation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of negative values: {0}\".format(len(df[df['Experience'] < 0])))\n",
    "\n",
    "df['Experience'] = df['Experience'].apply(lambda x : np.nan if x < 0 else x)\n",
    "\n",
    "print(\"Number of negative values after imputing with NaN: {0}\".format(len(df[df['Experience'] < 0])))\n",
    "print(\"Number of NANs: {0}\".format(df['Experience'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-state",
   "metadata": {},
   "source": [
    "### 1. Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-batch",
   "metadata": {},
   "source": [
    "#### 1.1 Outlier treatment for numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-designation",
   "metadata": {},
   "source": [
    "Outlier treatment for Non-Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with zero coding-Any value less than zero will be made zero\n",
    "def outliers_ZeroCoding(X,variable):\n",
    "    X.loc[X[variable]<0, variable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_quantile(df,variable):\n",
    "    # top coding: upper boundary for outliers according to interquantile proximity rule\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    Upper_fence = df[variable].quantile(0.75) + (IQR * 3)\n",
    "    df.loc[df[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_quantile(df,variable):\n",
    "    # bottom coding: lower boundary for outliers according to interquantile proximity rule\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    Lower_fence = df[variable].quantile(0.25) - (IQR * 3)\n",
    "    df.loc[df[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_normal_attributes:\n",
    "    outliers_TopCoding_quantile(df,col)\n",
    "    outliers_BottomCoding_quantile(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-secretary",
   "metadata": {},
   "source": [
    "Outlier treatment for Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_gaussian(df,variable):\n",
    "    # top coding: upper boundary for outliers according to gaussian rule\n",
    "    Upper_fence = df[variable].mean()+3*df[variable].std()\n",
    "    df.loc[df[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_gaussian(df,variable):\n",
    "    # bottom coding: lower boundary for outliers according to gaussian rule\n",
    "    Lower_fence = df[variable].mean()-3*df[variable].std()\n",
    "    df.loc[df[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in normal_attributes:\n",
    "    outliers_TopCoding_gaussian(df,col)\n",
    "    outliers_BottomCoding_gaussian(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-savage",
   "metadata": {},
   "source": [
    "Convert the non-normal distribution to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_BoxCox(df,variable):\n",
    "    df[variable+'_boxcox'], param = stats.boxcox(df[variable])\n",
    "    print('Optimal lambda: ', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-coordination",
   "metadata": {},
   "source": [
    "#### 1.2 Outlier treatment for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_new_imputation(df,variable,rare_cat):\n",
    "    temp = df.groupby([variable])[variable].count()/np.float(len(df))\n",
    "    rare_cat = [x for x in temp.loc[temp<0.05].index.values]\n",
    "    df[variable+'_rare_imp'] = np.where(df[variable].isin(rare_cat), 'Others', df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_freq_imputation(df,variable,rare_cat,frequent_cat):\n",
    "    # create new variables, with freq labels imputed\n",
    "    # by the most frequent category\n",
    "    df[variable+'_freq_imp'] = np.where(df[variable].isin(rare_cat), frequent_cat, df[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-carolina",
   "metadata": {},
   "source": [
    "### 2. Missing Values Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-zealand",
   "metadata": {},
   "source": [
    "#### 2.1 Imputation for numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors\n",
    "def impute_model_basic(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    for col in cols_nan:\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors,\n",
    "#   including progressively added imputed features\n",
    "def impute_model_progressive(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    while len(cols_nan) > 0:\n",
    "        col = cols_nan[0]\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "        cols_nan = df.columns[df.isna().any()].tolist()\n",
    "        cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for imputing missing data according to a given impute_strategy:\n",
    "#  drop_rows: drop all rows with one or more missing values\n",
    "#  drop_cols: drop columns with one or more missing values\n",
    "#  model_basic: KNN-model-based imputation with fixed predictors\n",
    "#  model_progressive: KNN-model-based imputation with progressively added predictors\n",
    "#  mean, median, most_frequent: imputation with mean, median or most frequent values\n",
    "#\n",
    "#  cols_to_standardize: if provided, the specified columns are scaled between 0 and 1, after imputation\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'drop_rows':\n",
    "        df = df.dropna(axis=0)\n",
    "    elif impute_strategy == 'drop_cols':\n",
    "        df = df.dropna(axis=1)\n",
    "    elif impute_strategy == 'model_basic':\n",
    "        df = impute_model_basic(df)\n",
    "    elif impute_strategy == 'model_progressive':\n",
    "        df = impute_model_progressive(df)\n",
    "    else:\n",
    "        arr = SimpleImputer(missing_values=np.nan,strategy=impute_strategy).fit(\n",
    "          df.values).transform(df.values)\n",
    "        df = pd.DataFrame(data=arr, index=df.index.values, columns=df.columns.values)\n",
    "    if cols_to_standardize != None:\n",
    "        cols_to_standardize = list(set(cols_to_standardize) & set(df.columns.values))\n",
    "        df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "        df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "          df[cols_to_standardize]).transform(df[cols_to_standardize]),\n",
    "                                             index=df[cols_to_standardize].index.values,\n",
    "                                             columns=df[cols_to_standardize].columns.values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_attributes] = impute_data(df[numerical_attributes], 'model_progressive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-monitoring",
   "metadata": {},
   "source": [
    "#### 2.2 Imputation for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na_freq(df, variable):\n",
    "    # find out most frequent category\n",
    "    most_frequent_category = df.groupby([variable])[variable].count().sort_values(ascending=False).index[0] \n",
    "    \n",
    "    ## replace missing values with most frequent category\n",
    "    df[variable].fillna(most_frequent_category, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na_addCat(df, variable):\n",
    "    if((df[variable].isnull().sum())>0):\n",
    "        df[variable+'_NA'] = np.where(df[variable].isnull(), 'Missing', df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in categorical_attributes:\n",
    "    impute_na_addCat(df,cols)\n",
    "    impute_na_freq(df,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-mercy",
   "metadata": {},
   "source": [
    "### 3. Encoding of categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalEncoding_OneHot(df,variable):\n",
    "    return pd.get_dummies(df, columns=[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using weight of evidence encoding technique\n",
    "def CategoricalEncoding_WOE(df,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = df.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df[target_variable] == 0, target_variable] = 0.001\n",
    "    prob_df['WoE'] = np.log(prob_df[target_variable]/prob_df['target_0'])\n",
    "    ordered_labels = prob_df['WoE'].to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalEncoding_RiskFactor(df,variable,target_variable):\n",
    "    ordered_labels = df.groupby([variable])[target_variable].mean().to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalEncoding_Monotonicity(df,variable,target_variable):\n",
    "    ordered_labels=df.groupby([variable])[target_variable].mean().sort_values().index\n",
    "    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 1)}\n",
    "    df[variable+'_ordered']=df[variable].map(ordinal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalEncoding_PRE(df,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = df.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df['target_0'] == 0, 'target_0'] = 0.001\n",
    "    prob_df['PRE'] = prob_df[target_variable]/prob_df['target_0']\n",
    "    ordered_labels = prob_df['PRE'].to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-walker",
   "metadata": {},
   "source": [
    "Sample dataset categorical attributes is already encoded, hence no need for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Family', 'Education']:\n",
    "#      df = CategoricalEncoding_OneHot(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-spirituality",
   "metadata": {},
   "source": [
    "### 4. Scaling of Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Standard Scalar: z = (x - x_mean) / std\n",
    "def scaler_Standard(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for standardisation\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Mean Normalisation: z=(x-x_mean)/(x_max-x_min)\n",
    "def scaler_MeanNormalisation(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    means = df_x.mean(axis=0)\n",
    "    ranges = df_x.max(axis=0)-df_x.min(axis=0)\n",
    "    df_scaled = (df_x - means) / ranges\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.MinMaxScaling:x_scaled=(x-x_min)/(x_max-x_min)\n",
    "def scaler_MinMax(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.MaxAbsScaling:x_scaled=x/x_max\n",
    "def scaler_MaxAbs(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    # set up the scaler\n",
    "    scaler = MaxAbsScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.RobustScaling:x_scaled = x - x_median / ( x.quantile(0.75) - x.quantile(0.25) )\n",
    "def scaler_Robust(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    # set up the scaler\n",
    "    scaler = RobustScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    \n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = scaler_Robust(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-toolbox",
   "metadata": {},
   "source": [
    "### 5. Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-spoke",
   "metadata": {},
   "source": [
    "1. Under Sampling\n",
    "* refer notebook example [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-01-Random-Undersampling.ipynb)\n",
    "* documentation [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "married-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imblearn random under sampling\n",
    "# params: strategy: 'majority'(it specifies to undersample majority class to have 1:1 ratio)\n",
    "#         strategy: 0.5 (resultant ratio will be 1:0.5)\n",
    "\n",
    "def apply_random_undersampling(df, strategy='auto'):\n",
    "    # define oversampling strategy\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=strategy,  # 'auto' - samples only the majority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        replacement=True # if it should resample with replacement\n",
    "    )  \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_under, y_train_under = rus.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_under[target] = y_train_under\n",
    "    return x_train_under"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-village",
   "metadata": {},
   "source": [
    "2. Condensed Nearest Neighbour\n",
    "* refere example notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-02-Condensed-Nearest-Neighbours.ipynb)\n",
    "* refer doc here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cnn_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    cnn = CondensedNearestNeighbour(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        random_state=0,            # for reproducibility\n",
    "        n_neighbors=1,             # default\n",
    "        n_jobs=4                   # I have 4 cores in my laptop\n",
    "    )   \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = cnn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-trace",
   "metadata": {},
   "source": [
    "3. Tomet Link\n",
    "* refere notebook example [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-03-Tomek-Links.ipynb)\n",
    "* refer doc here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tometlink_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    tl = TomekLinks(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        n_jobs=4                   # I have 4 cores in my laptop\n",
    "    )    \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = tl.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-indie",
   "metadata": {},
   "source": [
    "4. One Sided Selection\n",
    "* refer notebook example [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-04-One-Sised-Selection.ipynb)\n",
    "* refer doc here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "english-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_onesidedselection_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    oss = OneSidedSelection(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        random_state=0,            # for reproducibility\n",
    "        n_neighbors=1,             # default\n",
    "        n_jobs=4                   # I have 4 cores in my laptop\n",
    "    )   \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = oss.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-hazard",
   "metadata": {},
   "source": [
    "5. EditedNearestNeighbours\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-05-Edited-Nearest-Neighbours.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "preliminary-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_editednearestneighbour_undersampling(df, strategy='majority'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    enn = EditedNearestNeighbours(\n",
    "        sampling_strategy='auto',  # undersamples only the majority class\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',            # all neighbours need to have the same label as the observation examined\n",
    "        n_jobs=4                   # I have 4 cores in my laptop \n",
    "    ) \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = enn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-mirror",
   "metadata": {},
   "source": [
    "6. Repeated Edited Nearest Neighbours\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-05-Edited-Nearest-Neighbours.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consistent-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repeated_enn_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    renn = RepeatedEditedNearestNeighbours(\n",
    "        sampling_strategy=strategy, # removes only the majority class\n",
    "        n_neighbors=3,            # 3 KNN\n",
    "        kind_sel='all',           # all neighbouring observations should show the same class\n",
    "        n_jobs=4,                 # 4 processors in my laptop\n",
    "        max_iter=100              # maximum number of iterations \n",
    "    )\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = renn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-galaxy",
   "metadata": {},
   "source": [
    "7. All K Nearest Neighbours\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-07-All-KNN.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adopted-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_allknn_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    allknn = AllKNN(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',            # all neighbours need to have the same label as the observation examined\n",
    "        n_jobs=4                   # I have 4 cores in my laptop\n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = allknn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-retrieval",
   "metadata": {},
   "source": [
    "8. Neighbourd Cleaning Rule\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-08-Neighbourhood-Cleaning-Rule.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indonesian-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_neighbourhood_cleaning_rule_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    ncr = NeighbourhoodCleaningRule(\n",
    "        sampling_strategy=strategy,# removes only the majority class\n",
    "        n_neighbors=3,           # 3 KNN\n",
    "        kind_sel='all',          # all neighbouring observations should show the same class\n",
    "        n_jobs=4,                # 4 processors in my laptop\n",
    "        threshold_cleaning=0.5   # threshold no exclude or not observations\n",
    "    )   \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = ncr.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-casino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "centered-warren",
   "metadata": {},
   "source": [
    "9. NearMiss [v1|v2|v3]\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-09-NearMiss.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compliant-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nearmiss_v1_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    nm1 = NearMiss(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        version=1,\n",
    "        n_neighbors=3,\n",
    "        n_jobs=4                   # I have 4 cores in my laptop  \n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = nm1.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pediatric-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nearmiss_v2_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    nm2 = NearMiss(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        version=2,\n",
    "        n_neighbors=3,\n",
    "        n_jobs=4                   # I have 4 cores in my laptop  \n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = nm2.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "premier-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nearmiss_v3_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    nm3 = NearMiss(\n",
    "        sampling_strategy=strategy,  # undersamples only the majority class\n",
    "        version=3,\n",
    "        n_neighbors=3,\n",
    "        n_jobs=4                   # I have 4 cores in my laptop  \n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = nm3.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-angola",
   "metadata": {},
   "source": [
    "10. InstanceHardnessThreshold\n",
    "* refer notebook [here](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-04-Undersampling/04-10-Instance-Hardness-Class.ipynb)\n",
    "* refer doc [here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "double-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_instance_hardness_sampling_undersampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    iht = InstanceHardnessThreshold(\n",
    "        # TODO - review if we need to pass classifier as a parameter \n",
    "        # select a classifier, in this case Random Forests\n",
    "        estimator=RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "        sampling_strategy='auto',  # undersamples only the majority class\n",
    "        random_state=0,\n",
    "        n_jobs=4,                  # have 4 processors in my laptop\n",
    "        cv=3                       # cross validation fold \n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = iht.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "foster-mexico",
   "metadata": {},
   "source": [
    "<!-- #### COMPARISON\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON\n",
    "# ![image.png](attachment:image.png)\n",
    "# ![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-scoop",
   "metadata": {},
   "source": [
    "#### OVER SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-anthony",
   "metadata": {},
   "source": [
    "1. Random Over Sampler [Notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-01-Random-Oversampling.ipynb), [Document]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "later-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    ros = RandomOverSampler(\n",
    "        sampling_strategy=strategy, # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "    )  \n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-directive",
   "metadata": {},
   "source": [
    "2. SMOTE [Notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-02-SMOTE.ipynb) [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "electoral-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_oversampling(df, strategy='auto'):\n",
    "    sm = SMOTE(\n",
    "        random_state=42,\n",
    "        sampling_strategy=strategy,\n",
    "        k_neighbours=5,\n",
    "        n_jobs=4\n",
    "    )\n",
    "\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    x_train_sm, y_train_sm = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "    x_train_sm[target] = y_train_sm\n",
    "    return x_train_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-berkeley",
   "metadata": {},
   "source": [
    "3. SMOTE Nominal Continous for categorical data [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-03-SMOTE-NC.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "impressive-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smotenc_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    smnc = SMOTENC(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,            # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        n_jobs=4,\n",
    "        categorical_features=[2,3] # indeces of the columns of categorical variables\n",
    "    )    \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = smnc.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-fifty",
   "metadata": {},
   "source": [
    "4. ADASYN [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-04-ADASYN.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sonic-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adasyn_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    ada = ADASYN(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        n_neighbors=5,\n",
    "        n_jobs=4\n",
    "    )   \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = ada.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-student",
   "metadata": {},
   "source": [
    "5. BORDERLINE SMOTE [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-05-Borderline-SMOTE.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "compact-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_borderline1_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm_b1 = BorderlineSMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        m_neighbors=10,\n",
    "        kind='borderline-1',\n",
    "        n_jobs=4\n",
    "    )  \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = sm_b1.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "protected-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_borderline2_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm_b1 = BorderlineSMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        m_neighbors=10,\n",
    "        kind='borderline-2',\n",
    "        n_jobs=4\n",
    "    )  \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = sm_b1.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-fairy",
   "metadata": {},
   "source": [
    "6. SVM SMOTE [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-06-SVM-SMOTE.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beneficial-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svmsmote_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm = SVMSMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,              # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        m_neighbors=10,\n",
    "        n_jobs=4,\n",
    "        svm_estimator = svm.SVC(kernel='linear')\n",
    "    )  \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = sm.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-radiation",
   "metadata": {},
   "source": [
    "7. K-Means SMOTE [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-05-Oversampling/05-07-K-Means-SMOTE.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "engaged-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeanssmote_oversampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm = KMeansSMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,              # for reproducibility\n",
    "        k_neighbors=2,\n",
    "        n_jobs=None,\n",
    "        kmeans_estimator=KMeans(n_clusters=3, random_state=0),\n",
    "        cluster_balance_threshold=0.1,\n",
    "        density_exponent='auto'\n",
    "    )  \n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = sm.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-modification",
   "metadata": {},
   "source": [
    "#### COMBINATION OF UNDER AND OVER SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-environment",
   "metadata": {},
   "source": [
    "1. SMOTE + ENN [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-06-Over-and-Undersampling/06-01-SMOTEENN-and-SMOTETomek.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "after-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sm_enn_sampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm = SMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    # define under sampling strategy\n",
    "    # need ENN  as argument of SMOTEENN\n",
    "    enn = EditedNearestNeighbours(\n",
    "        sampling_strategy=strategy,\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=4)\n",
    "\n",
    "    smenn = SMOTEENN(\n",
    "        sampling_strategy='auto',  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        smote=sm,\n",
    "        enn=enn,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = smenn.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-treaty",
   "metadata": {},
   "source": [
    "2. SMOTE + Tomek [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-06-Over-and-Undersampling/06-01-SMOTEENN-and-SMOTETomek.ipynb), [Document]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "social-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sm_tomek_sampling(df, strategy='auto'):\n",
    "    \n",
    "    # define oversampling strategy\n",
    "    sm = SMOTE(\n",
    "        sampling_strategy=strategy,  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        k_neighbors=5,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    # define under sampling strategy\n",
    "    # need tomek as argument of SMOTETomek\n",
    "    tl = TomekLinks(\n",
    "        sampling_strategy='all',\n",
    "        n_jobs=4)\n",
    "\n",
    "    smtomek = SMOTETomek(\n",
    "        sampling_strategy='auto',  # samples only the minority class\n",
    "        random_state=0,  # for reproducibility\n",
    "        smote=sm,\n",
    "        tomek=tl,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    #separate train and test\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    \n",
    "    # fit and apply the transform\n",
    "    x_train_resampled, y_train_resampled = smtomek.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_resampled[target] = y_train_resampled\n",
    "    return x_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-delay",
   "metadata": {},
   "source": [
    "#### ENSEMBLE IMBALANCED LEARNING TECHNIQUE\n",
    "* *TODO: imblearn comes with model + sampling techniques, review to move this to more appropriate place* as these techniques is different from just the data sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "historic-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just re-sampling methods (no classifier)\n",
    "\n",
    "resampling_dict = {\n",
    "    \n",
    "    'random': RandomUnderSampler(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        replacement=False,\n",
    "    ),\n",
    "\n",
    "    'smote': SMOTE(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=4,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "continuing-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble methods (with or without resampling)\n",
    "\n",
    "ensemble_dict = {\n",
    "\n",
    "    # balanced random forests (bagging)\n",
    "    'balancedRF': BalancedRandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        criterion='gini',\n",
    "        max_depth=3,\n",
    "        sampling_strategy='auto',\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # bagging of Logistic regression, no resampling\n",
    "    'bagging': BaggingClassifier(\n",
    "        base_estimator=LogisticRegression(random_state=2909),\n",
    "        n_estimators=20,\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # bagging of Logistic regression, with resampling\n",
    "    'balancedbagging': BalancedBaggingClassifier(\n",
    "        base_estimator=LogisticRegression(random_state=2909),\n",
    "        n_estimators=20,\n",
    "        max_samples=1.0,  # The number of samples to draw from X to train each base estimator\n",
    "        max_features=1.0,  # The number of features to draw from X to train each base estimator\n",
    "        bootstrap=True,\n",
    "        bootstrap_features=False,\n",
    "        sampling_strategy='auto',\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # boosting + undersampling\n",
    "    'rusboost': RUSBoostClassifier(\n",
    "        base_estimator=None,\n",
    "        n_estimators=20,\n",
    "        learning_rate=1.0,\n",
    "        sampling_strategy='auto',\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # bagging + boosting + under-sammpling\n",
    "    'easyEnsemble': EasyEnsembleClassifier(\n",
    "        n_estimators=20,\n",
    "        sampling_strategy='auto',\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "front-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to train random forests and evaluate the peensembleormance\n",
    "\n",
    "# ensemble = ensemble_dict['choose_ensemble_technique']\n",
    "\n",
    "def run_ensemble(ensemble, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    ensemble.fit(X_train, y_train)\n",
    "\n",
    "    print('Train set')\n",
    "    pred = ensemble.predict_proba(X_train)\n",
    "    print(\n",
    "        'ensembleBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "\n",
    "    print('Test set')\n",
    "    pred = ensemble.predict_proba(X_test)\n",
    "    print(\n",
    "        'ensembleBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))\n",
    "\n",
    "    return roc_auc_score(y_test, pred[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-earthquake",
   "metadata": {},
   "source": [
    "#### Cost Sensitive learning approaches\n",
    "* **Misclassification cost as part of learning**\n",
    "    1. Defining the class_weight for those estimators that allow it, when we set the estimator. it can take values - |None|balanced|{0:1, and 1:10}(misclassification of class 1 will be penalized 10 times)|\n",
    "    2. Passing the sample_weight vector with the weights for every single observation, when we *fit the estimator*. Sample weight is the vector of the same length as y, containing the weight or penalty for each individual observation. It's more flexible as it allows us to set weight to the observation and not the classes.\n",
    "    NOTE: the costs such as 'class_weight' can be optimized using the hyperparameter optimization techniques.\n",
    "    \n",
    "* **MetaCost learning** - This is recent method and most likely has not been introduced in the popular libraries. Idea is to use the conditional risk of misclassifying the observations using Bayes Conditional Probabilities. For example refer [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/blob/master/Section-08-Cost-Sensitive-Learning/08-03-MetaCost.ipynb) and video in udemy course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-february",
   "metadata": {},
   "source": [
    "#### Probability Calibration\n",
    "* Refer slides [here](https://amueller.github.io/COMS4995-s20/slides/aml-10-calibration-imbalanced-data/#53) for understadning the topic\n",
    "* Refer [notebook](https://github.com/solegalli/machine-learning-imbalanced-data/tree/master/Section-09-Probability-Calibration) here for example for calibrated classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-taiwan",
   "metadata": {},
   "source": [
    "### 6. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_PCA(x_train):\n",
    "    variance = []\n",
    "    for num_components in range(2,len(x_train.columns)):\n",
    "        #print('Aplying PCA with',num_components,'components:')\n",
    "        pca = PCA(n_components=num_components)\n",
    "        pca.fit(x_train)\n",
    "        #print('explained variance ratio:',np.sum(pca.explained_variance_ratio_))\n",
    "        variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(2,len(x_train.columns)),variance)\n",
    "    plt.xlabel('num of components')\n",
    "    plt.ylabel('explained variance total')\n",
    "    plt.title('PCA components vs explained variance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_PCA(x_train, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_train_pca = pca.fit_transform(x_train)\n",
    "    x_train_pca = pd.DataFrame(x_train_pca)\n",
    "    return x_train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-poverty",
   "metadata": {},
   "source": [
    "### Save transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.to_excel('../Bank_Personal_Loan_Modelling_transformed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imblearn random under sampling\n",
    "# params: strategy: 'majority'(it specifies to undersample majority class to have 1:1 ratio)\n",
    "#         strategy: 0.5 (resultant ratio will be 1:0.5)\n",
    "\n",
    "def apply_undersampling(df, strategy='majority'):\n",
    "    # define oversampling strategy\n",
    "    undersample = RandomUnderSampler(sampling_strategy=strategy)\n",
    "    # fit and apply the transform\n",
    "    x_train = df.drop(columns=[target])\n",
    "    y_train=df[target]\n",
    "    x_train_under, y_train_under = undersample.fit_resample(x_train, y_train)\n",
    "    \n",
    "    # merging y to x\n",
    "    x_train_under[target] = y_train_under\n",
    "    return x_train_under"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
