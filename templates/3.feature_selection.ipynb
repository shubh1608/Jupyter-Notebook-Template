{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pacific-maria",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "* Filter method\n",
    "* Wrapper method\n",
    "* Embedded method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-wales",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "favorite-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import math\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "\n",
    "# using feature engine library\n",
    "from feature_engine.selection import (\n",
    "    DropCorrelatedFeatures, \n",
    "    SmartCorrelatedSelection,\n",
    "    DropConstantFeatures, \n",
    "    DropDuplicateFeatures,\n",
    "    SelectBySingleFeaturePerformance,\n",
    "    SelectByTargetMeanPerformance,\n",
    "    SelectByShuffling,\n",
    "    RecursiveFeatureElimination,\n",
    "    RecursiveFeatureAddition\n",
    ")\n",
    "\n",
    "# to obtain the mutual information values\n",
    "from sklearn.feature_selection import (\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif, \n",
    "    mutual_info_regression,\n",
    "    VarianceThreshold\n",
    ")\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "\n",
    "# wrapper methods\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "# embedded methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-statement",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effective-chance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/feature_selection/dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: note that we need to use only 3-4 techniques\n",
    "# to-do: we need to work on ensemble techniqye of above techniques features\n",
    "# to-do: we need to put variables in the top, identu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-tuning",
   "metadata": {},
   "source": [
    "### Splitting in to train and test set\n",
    "* its a good practice to select the features by examining only the training set. And this is to avoid the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "positive-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),  # drop the target\n",
    "    data['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-keyboard",
   "metadata": {},
   "source": [
    "## 1. Filter methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "constitutional-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install feature_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-blind",
   "metadata": {},
   "source": [
    "**Remove constand, quasi constant and duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "decimal-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_and_quasi_constant_features(df):\n",
    "    # remove constant and quasi-constant features first:\n",
    "    # we use Feature-engine for this\n",
    "    sel = DropConstantFeatures(tol=0.998, variables=None, missing_values='raise')\n",
    "    sel.fit(df)\n",
    "    return sel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "disciplinary-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 158)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_constant_and_quasi_constant_features(X_train)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "possible-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_features(df):\n",
    "    # set up the selector\n",
    "    sel = DropDuplicateFeatures(variables=None, missing_values='raise')\n",
    "    # find the duplicate features, this might take a while\n",
    "    sel.fit(df)\n",
    "    return sel.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "optimum-wealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 152)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_duplicate_features(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-procedure",
   "metadata": {},
   "source": [
    "**Remove Correlated features**\n",
    "*  \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bronze-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corr_features_brute_force(df, print_res=False):\n",
    "    sel = DropCorrelatedFeatures(\n",
    "        threshold=0.8,\n",
    "        method='pearson',\n",
    "        missing_values='ignore'\n",
    "    )\n",
    "    # find correlated features\n",
    "    sel.fit(df)\n",
    "    if (print_res):\n",
    "        sel.correlated_feature_sets_\n",
    "    return sel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "marked-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart correlation selection\n",
    "def remove_corr_features_smart(x_train, y_train, print_res=False):\n",
    "    # random forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=20,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    # correlation selector\n",
    "    sel = SmartCorrelatedSelection(\n",
    "        variables=None, # if none, selector examines all numerical variables\n",
    "        method=\"pearson\",\n",
    "        threshold=0.8,\n",
    "        missing_values=\"raise\",\n",
    "        selection_method=\"model_performance\", # this can be set to variance also to select feature with mst variance\n",
    "        estimator=rf,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=3,\n",
    "    )\n",
    "\n",
    "    # this may take a while, because we are training\n",
    "    # a random forest per correlation group\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    if (print_res):\n",
    "        sel.correlated_feature_sets_\n",
    "    return sel.fit_transform(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "unlimited-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 78)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_corr_features_smart(df, y_train)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-surge",
   "metadata": {},
   "source": [
    "**Statistical Techniques and Ranking Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-seeking",
   "metadata": {},
   "source": [
    "**Mutual Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "objective-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load classification dataset\n",
    "data_clf = pd.read_csv('../dataset/feature_selection/dataset_1.csv')\n",
    "data_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "peripheral-final",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "x_train_clf, x_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    data_clf.drop(labels=['target'], axis=1),\n",
    "    data_clf['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "x_train_clf.shape, x_test_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "great-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectkbest_mi_clf(x_train, y_train, k=10, print_res=False):\n",
    "    sel = SelectKBest(mutual_info_classif, k=k).fit(x_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        # display features\n",
    "        x_train.columns[sel.get_support()]\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ideal-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectkbest_mi_reg(x_train, y_train, percentile=10, print_res=False):\n",
    "    sel = SelectPercentile(mutual_info_regression, percentile=10).fit(X_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        # display features\n",
    "        x_train.columns[sel.get_support()]\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "employed-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = selectkbest_mi_clf(x_train_clf, y_train_clf)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-investor",
   "metadata": {},
   "source": [
    "**Chi-Square Test**\n",
    "NOTE: to be used only with the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "existing-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(x_train, y_train, k=1, print_res=False):\n",
    "    sel = SelectKBest(chi2, k=1).fit(x_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        # display features\n",
    "        x_train.columns[sel.get_support()]\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-vulnerability",
   "metadata": {},
   "source": [
    "**ANNOVA**\n",
    "* ANOVA assumes a linear relationship between the feature and the target and that the variables follow a Gaussian distribution. If this is not true, the result of this test may not be useful.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "israeli-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annova_clf(x_train, y_train, k=10, print_res=False):\n",
    "    # calculate the univariate statistical measure between\n",
    "    # each of the variables and the target\n",
    "\n",
    "    # similarly to chi2, the output is one array with f-scores\n",
    "    # and one array with the pvalues\n",
    "\n",
    "    sel = SelectKBest(f_classif, k=k).fit(x_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        # display features\n",
    "        print(x_train.columns[sel.get_support()])\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "qualified-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annova_reg(x_train, y_train, k=10, print_res=False):\n",
    "    # calculate the univariate statistical measure between\n",
    "    # each of the variables and the target\n",
    "\n",
    "    # similarly to chi2, the output is one array with f-scores\n",
    "    # and one array with the pvalues\n",
    "\n",
    "    sel = SelectPercentile(f_regression,\n",
    "                        percentile=10).fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        # display features\n",
    "        print(x_train.columns[sel.get_support()])\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "relevant-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_4', 'var_15', 'var_49', 'var_58', 'var_110', 'var_114', 'var_132',\n",
      "       'var_152', 'var_230', 'var_262'],\n",
      "      dtype='object')\n",
      "(35000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = annova_clf(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-empty",
   "metadata": {},
   "source": [
    "**Feature Selection with ML models**\n",
    "* Idea is that a single feature is taken out to build a model and then this feature will be ranked as per the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "responsible-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_single_feature_perf_clf(x_train, y_train, print_res=False):\n",
    "    # set up a machine learning model\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=10, random_state=1, n_jobs=4)\n",
    "\n",
    "    # set up the selector\n",
    "    sel = SelectBySingleFeaturePerformance(\n",
    "        variables=None,\n",
    "        estimator=rf,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=3,\n",
    "        threshold=0.5)\n",
    "\n",
    "    # find predictive features\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(sel.feature_performance_)\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "hindu-suffering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "{'var_1': 0.5001933142993099, 'var_2': 0.5, 'var_3': 0.5001784439685938, 'var_4': 0.6903562245974415, 'var_5': 0.501486791294873, 'var_6': 0.5, 'var_7': 0.5000594813228646, 'var_8': 0.49850669530475816, 'var_9': 0.5000594813228646, 'var_10': 0.5000148703307161, 'var_11': 0.5, 'var_12': 0.5, 'var_13': 0.4991720909314243, 'var_14': 0.5, 'var_15': 0.6674213182310633, 'var_16': 0.5003357973374138, 'var_17': 0.49557045024503094, 'var_18': 0.49809799867189103, 'var_19': 0.504767155890828, 'var_20': 0.5000594813228646, 'var_21': 0.6487429533835901, 'var_22': 0.5004418833556133, 'var_23': 0.5, 'var_24': 0.4997855951009161, 'var_25': 0.5011943413867188, 'var_26': 0.5013870201909277, 'var_27': 0.5033012134189864, 'var_28': 0.5000148703307161, 'var_29': 0.49609791811114484, 'var_30': 0.516004834198219, 'var_31': 0.5283737111160551, 'var_32': 0.5002911863452654, 'var_33': 0.5, 'var_34': 0.5, 'var_35': 0.6628156159398587, 'var_36': 0.5, 'var_37': 0.49632725205209066, 'var_38': 0.515148611361263, 'var_39': 0.4998976095301159, 'var_40': 0.5002613484921291, 'var_41': 0.5094762679622916, 'var_42': 0.49987399180456465, 'var_43': 0.49951792914802523, 'var_44': 0.5, 'var_45': 0.5004163692600524, 'var_46': 0.5109518670028926, 'var_47': 0.5153160096412405, 'var_48': 0.5005502022364977, 'var_49': 0.6492005974987188, 'var_50': 0.5135278297723684, 'var_51': 0.5048861185365573, 'var_52': 0.5166569599741508, 'var_53': 0.5004163692600524, 'var_54': 0.5048861185365573, 'var_55': 0.6461139100793968, 'var_56': 0.5005439819674401, 'var_57': 0.49106534589461887, 'var_58': 0.5249243082892737, 'var_59': 0.5001189626457292, 'var_60': 0.49953279947874146, 'var_61': 0.5, 'var_62': 0.5279831878463783, 'var_63': 0.5012829800085826, 'var_64': 0.5152565289550027, 'var_65': 0.5000148703307161, 'var_66': 0.5, 'var_67': 0.5, 'var_68': 0.5035986200333095, 'var_69': 0.5, 'var_70': 0.51080133636437, 'var_71': 0.5000594813228646, 'var_72': 0.5, 'var_73': 0.5, 'var_74': 0.55202048658856, 'var_75': 0.6531421392901454, 'var_76': 0.6365654864725864, 'var_77': 0.49994837592611646, 'var_78': 0.5004163692600524, 'var_79': 0.5092033591748164, 'var_80': 0.5, 'var_81': 0.5, 'var_82': 0.5199974469055048, 'var_83': 0.503781900244119, 'var_84': 0.5017143950254254, 'var_85': 0.4953408970922286, 'var_86': 0.5151661902564181, 'var_87': 0.5, 'var_88': 0.515148611361263, 'var_89': 0.5, 'var_90': 0.5001040923150131, 'var_91': 0.5126890838287937, 'var_92': 0.5, 'var_93': 0.5156138414515846, 'var_94': 0.5016995246947091, 'var_95': 0.5009712194386813, 'var_96': 0.4955022109876195, 'var_97': 0.5, 'var_98': 0.5003064778400322, 'var_99': 0.5, 'var_100': 0.49223026677249093, 'var_101': 0.49371799779937836, 'var_102': 0.5000830017152392, 'var_103': 0.5210945387966228, 'var_104': 0.5, 'var_105': 0.49205221878581784, 'var_106': 0.49951792914802523, 'var_107': 0.5247993101377553, 'var_108': 0.5076116652096457, 'var_109': 0.5070989599428323, 'var_110': 0.6318294130404496, 'var_111': 0.5000297406614322, 'var_112': 0.5, 'var_113': 0.5, 'var_114': 0.5248971391760279, 'var_115': 0.5005204615750655, 'var_116': 0.5000594813228646, 'var_117': 0.5177103750169657, 'var_118': 0.5053850604960665, 'var_119': 0.4991573504725921, 'var_120': 0.5, 'var_121': 0.5034033177604551, 'var_122': 0.5, 'var_123': 0.5210374872663589, 'var_124': 0.5, 'var_125': 0.5002465753531169, 'var_126': 0.49975506162680644, 'var_127': 0.5, 'var_128': 0.49972660299044236, 'var_129': 0.5, 'var_130': 0.50035066766813, 'var_131': 0.5121543586991673, 'var_132': 0.6928944405565499, 'var_133': 0.5, 'var_134': 0.5013347984658744, 'var_135': 0.5, 'var_136': 0.5001784439685938, 'var_137': 0.5007984145695379, 'var_138': 0.49890991503655724, 'var_139': 0.49960948670414435, 'var_140': 0.4980771957567316, 'var_141': 0.5, 'var_142': 0.5002974066143231, 'var_143': 0.5024600815393295, 'var_144': 0.5063696147214238, 'var_145': 0.5269115951961939, 'var_146': 0.5000297406614324, 'var_147': 0.5156459506479644, 'var_148': 0.49632725205209066, 'var_149': 0.49953200666606407, 'var_150': 0.5000148703307161, 'var_151': 0.5, 'var_152': 0.6760421536225688, 'var_153': 0.5, 'var_154': 0.5046481932450988, 'var_155': 0.5174429866333955, 'var_156': 0.4990035939041843, 'var_157': 0.5157996826001326, 'var_158': 0.5, 'var_159': 0.49965981418127764, 'var_160': 0.5211079567692115, 'var_161': 0.5251925455794234, 'var_162': 0.5166991411736986, 'var_163': 0.5210606917503272, 'var_164': 0.5019654789701226, 'var_165': 0.5035242683797287, 'var_166': 0.5124001517793184, 'var_167': 0.5, 'var_168': 0.5062639221386377, 'var_169': 0.5084918958122226, 'var_170': 0.5, 'var_171': 0.5, 'var_172': 0.504767155890828, 'var_173': 0.49721877167622575, 'var_174': 0.5087625964239886, 'var_175': 0.5301080660211751, 'var_176': 0.5075676003018895, 'var_177': 0.5017143950254254, 'var_178': 0.5, 'var_179': 0.5297581911657226, 'var_180': 0.5, 'var_181': 0.5096525481775854, 'var_182': 0.5, 'var_183': 0.5, 'var_184': 0.5002730287562743, 'var_185': 0.5005638307203756, 'var_186': 0.5142313486680984, 'var_187': 0.5, 'var_188': 0.5016791512904862, 'var_189': 0.5000297406614322, 'var_190': 0.5045338008730211, 'var_191': 0.5084699662796905, 'var_192': 0.49577439840610094, 'var_193': 0.5014572924101831, 'var_194': 0.5014870330716155, 'var_195': 0.5, 'var_196': 0.5, 'var_197': 0.49953279947874146, 'var_198': 0.5016995246947091, 'var_199': 0.5017143950254254, 'var_200': 0.5013687647028696, 'var_201': 0.5, 'var_202': 0.5009563167814668, 'var_203': 0.4940758139385604, 'var_204': 0.5001784439685939, 'var_205': 0.4961747509826311, 'var_206': 0.5052982181523191, 'var_207': 0.5079758568841083, 'var_208': 0.5070169669982562, 'var_209': 0.5101434080880877, 'var_210': 0.5, 'var_211': 0.49966663245518683, 'var_212': 0.5, 'var_213': 0.5035242683797287, 'var_214': 0.5088693891687637, 'var_215': 0.5, 'var_216': 0.49953279947874146, 'var_217': 0.5, 'var_218': 0.5011106915145772, 'var_219': 0.5004907209136331, 'var_220': 0.5001137444981204, 'var_221': 0.5000892219842968, 'var_222': 0.6508266417991039, 'var_223': 0.5, 'var_224': 0.5000362849028367, 'var_225': 0.5, 'var_226': 0.49658580101287303, 'var_227': 0.5, 'var_228': 0.5, 'var_229': 0.5035062255250492, 'var_230': 0.6825672213108328, 'var_231': 0.6530629780619418, 'var_232': 0.49658580101287303, 'var_233': 0.5, 'var_234': 0.5, 'var_235': 0.49953279947874146, 'var_236': 0.5004398896524269, 'var_237': 0.5002974066143231, 'var_238': 0.5014870330716155, 'var_239': 0.49953200666606407, 'var_240': 0.5007541929808325, 'var_241': 0.5150026268753067, 'var_242': 0.5020818463002618, 'var_243': 0.5, 'var_244': 0.4976973625841894, 'var_245': 0.4996447490427356, 'var_246': 0.49914873026166084, 'var_247': 0.5, 'var_248': 0.5, 'var_249': 0.5004398896524269, 'var_250': 0.5017143950254254, 'var_251': 0.5000978720459553, 'var_252': 0.49726608649347953, 'var_253': 0.5156647957232517, 'var_254': 0.4999632137888616, 'var_255': 0.502169011343102, 'var_256': 0.5019269227240424, 'var_257': 0.5001189626457293, 'var_258': 0.5166991411736986, 'var_259': 0.5177132928901761, 'var_260': 0.5004163692600524, 'var_261': 0.49665675640350954, 'var_262': 0.6925486028351031, 'var_263': 0.5000892219842968, 'var_264': 0.4997558544394837, 'var_265': 0.5000594813228646, 'var_266': 0.5029686263787311, 'var_267': 0.5000743516535808, 'var_268': 0.49911079932465174, 'var_269': 0.5035062255250492, 'var_270': 0.5035892272402885, 'var_271': 0.49801877144670065, 'var_272': 0.5095063553731659, 'var_273': 0.5007978288728062, 'var_274': 0.5001784439685938, 'var_275': 0.5087621750477288, 'var_276': 0.5050864372877714, 'var_277': 0.5056624795727939, 'var_278': 0.5019777539852487, 'var_279': 0.5086550201329941, 'var_280': 0.5, 'var_281': 0.49850669530475816, 'var_282': 0.4995406567277225, 'var_283': 0.5, 'var_284': 0.515346836600343, 'var_285': 0.5, 'var_286': 0.4998381605337496, 'var_287': 0.5, 'var_288': 0.5036730125724834, 'var_289': 0.5000594813228646, 'var_290': 0.5003568879371877, 'var_291': 0.5005439819674401, 'var_292': 0.49819831918483565, 'var_293': 0.499962304544201, 'var_294': 0.5, 'var_295': 0.5153475499053963, 'var_296': 0.5024600815393295, 'var_297': 0.5, 'var_298': 0.501023605130525, 'var_299': 0.5004163692600524, 'var_300': 0.5143483035341135}\n",
      "(35000, 246)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = select_by_single_feature_perf_clf(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "looking-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_single_feature_perf_reg(x_train, y_train, print_res=False):\n",
    "    # set up a machine learning model\n",
    "    # set up the machine learning model\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=10, max_depth=2, random_state=1, n_jobs=4)\n",
    "\n",
    "    # set up the selector\n",
    "    sel = SelectBySingleFeaturePerformance(\n",
    "        variables=None,\n",
    "        estimator=rf,\n",
    "        scoring=\"r2\",\n",
    "        cv=3,\n",
    "        threshold=0.5)\n",
    "\n",
    "    # find predictive features\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(sel.feature_performance_)\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "australian-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_target_mean_perf_reg(x_train, y_train, print_res=False):\n",
    "    # feautre engine automates the selection for both\n",
    "    # categorical and numerical variables\n",
    "\n",
    "    sel = SelectByTargetMeanPerformance(\n",
    "        variables=None, # automatically finds categorical and numerical variables\n",
    "        scoring=\"roc_auc_score\", # the metric to evaluate performance\n",
    "        threshold=0.6, # the threshold for feature selection, \n",
    "        bins=3, # the number of intervals to discretise the numerical variables\n",
    "        strategy=\"equal_frequency\", # whether the intervals should be of equal size or equal number of observations\n",
    "        cv=2,# cross validation\n",
    "        random_state=1, #seed for reproducibility\n",
    "    )\n",
    "\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(sel.feature_performance_)\n",
    "        \n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-toner",
   "metadata": {},
   "source": [
    "## 2. Wrapper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "associate-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\shubham\\appdata\\roaming\\python\\python37\\site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\shubham\\appdata\\roaming\\python\\python37\\site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from mlxtend) (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shubham\\.conda\\envs\\finance\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "coated-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward_selection_clf(x_train, y_train, k = 10, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    sfs = SFS(RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0), \n",
    "           k_features=k, # the more features we want, the longer it will take to run\n",
    "           forward=True, \n",
    "           floating=False, # see the docs for more details in this parameter\n",
    "           verbose=2, # this indicates how much to print out intermediate steps\n",
    "           scoring='roc_auc',\n",
    "           cv=2)\n",
    "\n",
    "    sfs = sfs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(sfs.k_feature_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "alternate-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward_selection_reg(x_train, y_train, k = 10, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    sfs = SFS(RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), \n",
    "           k_features=20, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=2)\n",
    "\n",
    "    sfs = sfs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(sfs.k_feature_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fixed-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_backward_selection_clf(x_train, y_train, k = 10, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    sfs = SFS(RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0), \n",
    "           k_features=k, # the more features we want, the longer it will take to run\n",
    "           forward=False, \n",
    "           floating=False, # see the docs for more details in this parameter\n",
    "           verbose=2, # this indicates how much to print out intermediate steps\n",
    "           scoring='roc_auc',\n",
    "           cv=2)\n",
    "\n",
    "    sfs = sfs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(sfs.k_feature_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "rural-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_backward_selection_reg(x_train, y_train, k = 10, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    sfs = SFS(RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), \n",
    "           k_features=20, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=2)\n",
    "\n",
    "    sfs = sfs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(sfs.k_feature_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "attractive-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_selection_clf(x_train, y_train, min_features=1, max_features=2, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    efs = EFS(RandomForestClassifier(n_estimators=5, n_jobs=4, random_state=0, max_depth=2),\n",
    "              min_features=min_features,\n",
    "              max_features=max_features,\n",
    "              scoring='roc_auc',\n",
    "              print_progress=True,\n",
    "              cv=2)\n",
    "\n",
    "    # search features\n",
    "    efs = efs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(efs.best_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "stone-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_selection_reg(x_train, y_train, min_features=1, max_features=2, print_res=False):\n",
    "    # review to increase the n_estimators\n",
    "    efs = EFS(RandomForestRegressor(n_estimators=5, n_jobs=4, random_state=0, max_depth=2),\n",
    "              min_features=min_features,\n",
    "              max_features=max_features,\n",
    "              scoring='r2',\n",
    "              print_progress=True,\n",
    "              cv=2)\n",
    "\n",
    "    # search features\n",
    "    efs = efs.fit(np.array(x_train), y_train)\n",
    "    selected_feat = x_train.columns[list(efs.best_idx_)]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "        \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-outdoors",
   "metadata": {},
   "source": [
    "## Embedded Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "brilliant-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_selection(x_train, y_train, print_res=False):\n",
    "    sel = SelectFromModel(LogisticRegression(C=1000, penalty='l2', max_iter=300, random_state=10))\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    \n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "incorporate-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_selection(x_train, y_train, print_res=False):\n",
    "    sel = SelectFromModel(LinearRegression())\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    \n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "endangered-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_lasso_selection(x_train, y_train, print_res=True):\n",
    "    sel = SelectFromModel(LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    \n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    \n",
    "    return x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "complimentary-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_6', 'var_14', 'var_21', 'var_29', 'var_30', 'var_34', 'var_35',\n",
      "       'var_46', 'var_47', 'var_55', 'var_60', 'var_64', 'var_75', 'var_76',\n",
      "       'var_77', 'var_79', 'var_86', 'var_98', 'var_108', 'var_111', 'var_124',\n",
      "       'var_125', 'var_136', 'var_142', 'var_147', 'var_151', 'var_154',\n",
      "       'var_183', 'var_213', 'var_216', 'var_217', 'var_221', 'var_222',\n",
      "       'var_231', 'var_253', 'var_257', 'var_263', 'var_300'],\n",
      "      dtype='object')\n",
      "(35000, 38)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = log_reg_selection(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "controlled-musician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_4', 'var_5', 'var_8', 'var_13', 'var_14', 'var_15', 'var_17',\n",
      "       'var_18', 'var_21', 'var_22',\n",
      "       ...\n",
      "       'var_271', 'var_277', 'var_279', 'var_280', 'var_286', 'var_288',\n",
      "       'var_292', 'var_296', 'var_299', 'var_300'],\n",
      "      dtype='object', length=117)\n",
      "(35000, 117)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = log_reg_lasso_selection(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "devoted-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_selection_clf(x_train, y_train, print_res=False):\n",
    "    sel = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flexible-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_4', 'var_15', 'var_17', 'var_21', 'var_29', 'var_35', 'var_46',\n",
      "       'var_49', 'var_50', 'var_55', 'var_57', 'var_74', 'var_75', 'var_76',\n",
      "       'var_110', 'var_131', 'var_132', 'var_140', 'var_145', 'var_152',\n",
      "       'var_157', 'var_161', 'var_166', 'var_173', 'var_185', 'var_190',\n",
      "       'var_203', 'var_207', 'var_220', 'var_222', 'var_231', 'var_255',\n",
      "       'var_261', 'var_262', 'var_272'],\n",
      "      dtype='object')\n",
      "(35000, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = random_forest_selection_clf(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suffering-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_selection_reg(x_train, y_train, print_res=False):\n",
    "    sel = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-tractor",
   "metadata": {},
   "source": [
    "*Below methods handle the correlated features well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_selection_clf_rfe(x_train, y_train, print_res=False):\n",
    "    sel = RFE(RandomForestClassifier(n_estimators=10, random_state=10), n_features_to_select=27)\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "falling-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_selection_reg_rfe(x_train, y_train, print_res=False):\n",
    "    sel = RFE(RandomForestRegressor(n_estimators=10, random_state=10), n_features_to_select=27)\n",
    "\n",
    "    sel.fit(x_train, y_train)\n",
    "    selected_feat = x_train.columns[(sel.get_support())]\n",
    "    if print_res:\n",
    "        print(selected_feat)\n",
    "    return sel.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "possible-flavor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_4', 'var_17', 'var_21', 'var_29', 'var_31', 'var_35', 'var_46',\n",
      "       'var_49', 'var_50', 'var_55', 'var_74', 'var_75', 'var_76', 'var_91',\n",
      "       'var_93', 'var_110', 'var_145', 'var_157', 'var_161', 'var_173',\n",
      "       'var_185', 'var_190', 'var_203', 'var_207', 'var_222', 'var_231',\n",
      "       'var_266'],\n",
      "      dtype='object')\n",
      "(35000, 27)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = random_forest_selection_clf_rfe(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-pantyhose",
   "metadata": {},
   "source": [
    "## Hybrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-question",
   "metadata": {},
   "source": [
    "**Feature Shuffling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "geological-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classifier, can be any classifier, chossing RF as a good default classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "medium-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shuffling_selection_clf(x_train, y_train, model=rf,print_res=False):\n",
    "    sel = SelectByShuffling(\n",
    "        variables=None, # automatically examine all numerical variables\n",
    "        estimator=model, # the ML model\n",
    "        scoring='roc_auc', # the metric to evaluate\n",
    "        threshold=0,# the maximum performance drop allowed to select the feature\n",
    "        cv=3, # cross validation\n",
    "        random_state=1 # seed\n",
    "    )\n",
    "\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    df = sel.transform(x_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "successful-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 300)\n",
      "Index(['var_4', 'var_21', 'var_30', 'var_35', 'var_46', 'var_55', 'var_75',\n",
      "       'var_76', 'var_82', 'var_107', 'var_110', 'var_132', 'var_152',\n",
      "       'var_161', 'var_203', 'var_205', 'var_222', 'var_230', 'var_231',\n",
      "       'var_262'],\n",
      "      dtype='object')\n",
      "(35000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "df = feature_shuffling_selection_clf(X_train, y_train, print_res=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "convertible-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here again we can choose any other regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100,\n",
    "                           max_depth=3,\n",
    "                           random_state=2909,\n",
    "                           n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rising-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shuffling_selection_reg(x_train, y_train, model=rf_reg,print_res=False):\n",
    "    sel = SelectByShuffling(\n",
    "        variables=None, # automatically examine all numerical variables\n",
    "        estimator=model, # the ML model\n",
    "        scoring='neg_root_mean_squared_error', # the metric to evaluate\n",
    "        threshold=None,# the maximum performance drop allowed to select the feature\n",
    "        cv=3, # cross validation\n",
    "        random_state=1 # seed\n",
    "    )\n",
    "\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    df = sel.transform(x_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-garbage",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "first-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ML model for which we want to select features\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=2,\n",
    "    random_state=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hollow-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_selection_clf(x_train, y_train, model = model,print_res=False):\n",
    "    # Setup the RFE selector\n",
    "    sel = RecursiveFeatureElimination(\n",
    "        variables=None, # automatically evaluate all numerical variables\n",
    "        estimator = model, # the ML model\n",
    "        scoring = 'roc_auc', # the metric we want to evalute\n",
    "        threshold = 0.0005, # the maximum performance drop allowed to remove a feature\n",
    "        cv=2, # cross-validation\n",
    "    )\n",
    "\n",
    "    # this may take quite a while, because\n",
    "    # we are building a lot of models with cross-validation\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    df = sel.transform(x_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "happy-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build initial model using all the features\n",
    "model = GradientBoostingRegressor(n_estimators=10, max_depth=4, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cognitive-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_selection_reg(x_train, y_train, model = model,print_res=False):\n",
    "    # Setup the RFE selector\n",
    "\n",
    "    sel = RecursiveFeatureElimination(\n",
    "        variables=None, # automatically evaluate all numerical variables\n",
    "        estimator = model, # the ML model\n",
    "        scoring = 'r2', # the metric we want to evalute\n",
    "        threshold = 0.001, # the maximum performance drop allowed to remove a feature\n",
    "        cv=3, # cross-validation\n",
    "    )\n",
    "\n",
    "    # this may take quite a while, because\n",
    "    # we are building a lot of models with cross-validation\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    df = sel.transform(x_train)\n",
    "    \n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-sudan",
   "metadata": {},
   "source": [
    "**Recursive Feature Addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "technical-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ML model for which we want to select features\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=2,\n",
    "    random_state=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "trying-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfa_selection_clf(x_train, y_train, model=model, print_res=False):\n",
    "    # Setup the RFA selector\n",
    "\n",
    "    rfa = RecursiveFeatureAddition(\n",
    "        variables=None,  # automatically evaluate all numerical variables\n",
    "        estimator=model,  # the ML model\n",
    "        scoring='roc_auc',  # the metric we want to evalute\n",
    "        threshold=0.0001,  # the minimum performance increase needed to select a feature\n",
    "        cv=2,  # cross-validation\n",
    "    )\n",
    "\n",
    "    rfa.fit(X_train, y_train)\n",
    "    df = rfa.transform(x_train)\n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "foreign-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model for which we want to select features\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=10, max_depth=4, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "prescribed-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfa_selection_reg(x_train, y_train, model=model, print_res=False):\n",
    "    # Setup the RFA selector\n",
    "    rfa = RecursiveFeatureAddition(\n",
    "        variables=None,  # automatically evaluate all numerical variables\n",
    "        estimator=model,  # the ML model\n",
    "        scoring='r2',  # the metric we want to evalute\n",
    "        threshold=0.001,  # the minimum performance increase needed to select a feature\n",
    "        cv=2,  # cross-validation\n",
    "    )\n",
    "\n",
    "    rfa.fit(X_train, y_train)\n",
    "    df = rfa.transform(x_train)\n",
    "    if print_res:\n",
    "        print(df.columns)\n",
    "        \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
