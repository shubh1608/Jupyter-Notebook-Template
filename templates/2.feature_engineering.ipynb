{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-nepal",
   "metadata": {},
   "source": [
    "## Feature Engineering Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-skiing",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weekly-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import math\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    CondensedNearestNeighbour,\n",
    "    TomekLinks,\n",
    "    OneSidedSelection,\n",
    "    EditedNearestNeighbours,\n",
    "    RepeatedEditedNearestNeighbours,\n",
    "    AllKNN,\n",
    "    NeighbourhoodCleaningRule,\n",
    "    NearMiss,\n",
    "    InstanceHardnessThreshold\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    "    SMOTE,\n",
    "    ADASYN,\n",
    "    BorderlineSMOTE,\n",
    "    SVMSMOTE,\n",
    ")\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "from imblearn.ensemble import (\n",
    "    BalancedBaggingClassifier,\n",
    "    BalancedRandomForestClassifier,\n",
    "    RUSBoostClassifier,\n",
    "    EasyEnsembleClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "\n",
    "\n",
    "# adding common folder location to sys.path\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "\n",
    "from helper import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-salad",
   "metadata": {},
   "source": [
    "### Loading Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-investment",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "korean-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your dataset, can be a csv file or xlsx\n",
    "dataset_path = \"../dataset/Bank_Personal_Loan_Modelling.xlsx\"\n",
    "\n",
    "## use code as per the type of data source\n",
    "\n",
    "## use below line to read data from csv file\n",
    "## df = pd.read_csv(dataset_path)\n",
    "df = pd.read_excel(dataset_path, sheet_name = 1, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "ID                                                                          \n",
       "1    25           1      49     91107       4    1.6          1         0   \n",
       "2    45          19      34     90089       3    1.5          1         0   \n",
       "3    39          15      11     94720       1    1.0          1         0   \n",
       "4    35           9     100     94112       1    2.7          2         0   \n",
       "5    35           8      45     91330       4    1.0          2         0   \n",
       "\n",
       "    Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "ID                                                                     \n",
       "1               0                   1           0       0           0  \n",
       "2               0                   1           0       0           0  \n",
       "3               0                   0           0       0           0  \n",
       "4               0                   0           0       0           0  \n",
       "5               0                   0           0       0           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amazing-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Personal Loan'\n",
    "# df_x = df.drop(columns=[target])\n",
    "# df_y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-relaxation",
   "metadata": {},
   "source": [
    "Let's separate out, numerical, categorical and numerical_normal and numerical_non_normal attributes<br/>\n",
    "This list will be used for outliers treatment and other transformation later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-vermont",
   "metadata": {},
   "source": [
    "### Separating Numerical and Categorical attributes along with normal and non normal numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollow-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Loan           2\n",
      "Securities Account      2\n",
      "CD Account              2\n",
      "Online                  2\n",
      "CreditCard              2\n",
      "Education               3\n",
      "Family                  4\n",
      "Age                    45\n",
      "Experience             47\n",
      "CCAvg                 108\n",
      "Income                162\n",
      "Mortgage              347\n",
      "ZIP Code              467\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# value counts method can be used to see if an attribute contains categorical data or continous data\n",
    "unique_val_in_cols = df.apply( lambda col : col.nunique()).sort_values()\n",
    "print(unique_val_in_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "historical-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide reasnable threshold value for separating categorical and numerical attributes based on above result\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patient-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: add columsn for categorical and numerical in the dataset df, and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes = list(unique_val_in_cols[unique_val_in_cols < threshold].keys())\n",
    "numerical_attributes = list(unique_val_in_cols[unique_val_in_cols > threshold].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_test(df, significance = .01):\n",
    "    \"\"\"\n",
    "    Function to perform ks test and test against normal distribution using  \n",
    "    D’Agostino, R. B. (1971), “An omnibus test of normality for moderate and large sample size”\n",
    "    \n",
    "    frame: a pandas dataframe\n",
    "    significance: float. Alpha level for which the null hypotesis will be rejected (H0: series comes from a normal distribution)\n",
    "    plot: Boolean, whether or not plot a histogram for resulting columns\n",
    "    \n",
    "    returns a dataframe with only those columns that follow a normal distribution according to test.\n",
    "\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    non_normal_columns = []\n",
    "\n",
    "    for col in columns:\n",
    "        aux = df[col]\n",
    "    \n",
    "        _, p = normaltest(aux)\n",
    "    \n",
    "        if p <= significance:\n",
    "            # col is not normally distributed\n",
    "            non_normal_columns.append(col)\n",
    "        \n",
    "    normal_columns = [cols for cols in columns if cols not in non_normal_columns]\n",
    "    return normal_columns, non_normal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latest-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_attributes, non_normal_attributes = normal_test(df[numerical_attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acute-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes:\n",
      "categorical:7\n",
      "numerical:6\n",
      "normal attributes:0\n",
      "non normal attributes:6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes:\")\n",
    "print(\"categorical:{0}\".format(len(categorical_attributes)))\n",
    "print(\"numerical:{0}\".format(len(numerical_attributes)))\n",
    "print(\"normal attributes:{0}\".format(len(normal_attributes)))\n",
    "print(\"non normal attributes:{0}\".format(len(non_normal_attributes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-category",
   "metadata": {},
   "source": [
    "#### Removing Target column from the categorical list of variables so that it does not get transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "removable-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes.remove(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-pierre",
   "metadata": {},
   "source": [
    "### Perform basic data cleaning as per observations from EDA\n",
    "* For this dataset, we know that the 'Experience' column has minor percentage of negative values(which is wrong) so let's treat it before using any other transformation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "annual-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative values: 52\n",
      "Number of negative values after imputing with NaN: 0\n",
      "Number of NANs: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negative values: {0}\".format(len(df[df['Experience'] < 0])))\n",
    "\n",
    "df['Experience'] = df['Experience'].apply(lambda x : np.nan if x < 0 else x)\n",
    "\n",
    "print(\"Number of negative values after imputing with NaN: {0}\".format(len(df[df['Experience'] < 0])))\n",
    "print(\"Number of NANs: {0}\".format(df['Experience'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-state",
   "metadata": {},
   "source": [
    "### 1. Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-batch",
   "metadata": {},
   "source": [
    "#### 1.1 Outlier treatment for numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-designation",
   "metadata": {},
   "source": [
    "Outlier treatment for Non-Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unavailable-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with zero coding-Any value less than zero will be made zero\n",
    "def outliers_ZeroCoding(X,variable):\n",
    "    X.loc[X[variable]<0, variable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "through-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_quantile(df,variable):\n",
    "    # top coding: upper boundary for outliers according to interquantile proximity rule\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    Upper_fence = df[variable].quantile(0.75) + (IQR * 3)\n",
    "    df.loc[df[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "furnished-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_quantile(df,variable):\n",
    "    # bottom coding: lower boundary for outliers according to interquantile proximity rule\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    Lower_fence = df[variable].quantile(0.25) - (IQR * 3)\n",
    "    df.loc[df[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "least-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_normal_attributes:\n",
    "    outliers_TopCoding_quantile(df,col)\n",
    "    outliers_BottomCoding_quantile(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-secretary",
   "metadata": {},
   "source": [
    "Outlier treatment for Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "artistic-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_gaussian(df,variable):\n",
    "    # top coding: upper boundary for outliers according to gaussian rule\n",
    "    Upper_fence = df[variable].mean()+3*df[variable].std()\n",
    "    df.loc[df[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dramatic-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_gaussian(df,variable):\n",
    "    # bottom coding: lower boundary for outliers according to gaussian rule\n",
    "    Lower_fence = df[variable].mean()-3*df[variable].std()\n",
    "    df.loc[df[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "monetary-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in normal_attributes:\n",
    "    outliers_TopCoding_gaussian(df,col)\n",
    "    outliers_BottomCoding_gaussian(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-savage",
   "metadata": {},
   "source": [
    "Convert the non-normal distribution to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "announced-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_BoxCox(df,variable):\n",
    "    df[variable+'_boxcox'], param = stats.boxcox(df[variable])\n",
    "    print('Optimal lambda: ', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-coordination",
   "metadata": {},
   "source": [
    "#### 1.2 Outlier treatment for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "atomic-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_new_imputation(df,variable,rare_cat):\n",
    "    temp = df.groupby([variable])[variable].count()/np.float(len(df))\n",
    "    rare_cat = [x for x in temp.loc[temp<0.05].index.values]\n",
    "    df[variable+'_rare_imp'] = np.where(df[variable].isin(rare_cat), 'Others', df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "strong-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_freq_imputation(df,variable,rare_cat,frequent_cat):\n",
    "    # create new variables, with freq labels imputed\n",
    "    # by the most frequent category\n",
    "    df[variable+'_freq_imp'] = np.where(df[variable].isin(rare_cat), frequent_cat, df[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-carolina",
   "metadata": {},
   "source": [
    "### 2. Missing Values Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-zealand",
   "metadata": {},
   "source": [
    "#### 2.1 Imputation for numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "after-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors\n",
    "def impute_model_basic(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    for col in cols_nan:\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fifty-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors,\n",
    "#   including progressively added imputed features\n",
    "def impute_model_progressive(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    while len(cols_nan) > 0:\n",
    "        col = cols_nan[0]\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "        cols_nan = df.columns[df.isna().any()].tolist()\n",
    "        cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "natural-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for imputing missing data according to a given impute_strategy:\n",
    "#  drop_rows: drop all rows with one or more missing values\n",
    "#  drop_cols: drop columns with one or more missing values\n",
    "#  model_basic: KNN-model-based imputation with fixed predictors\n",
    "#  model_progressive: KNN-model-based imputation with progressively added predictors\n",
    "#  mean, median, most_frequent: imputation with mean, median or most frequent values\n",
    "#\n",
    "#  cols_to_standardize: if provided, the specified columns are scaled between 0 and 1, after imputation\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'drop_rows':\n",
    "        df = df.dropna(axis=0)\n",
    "    elif impute_strategy == 'drop_cols':\n",
    "        df = df.dropna(axis=1)\n",
    "    elif impute_strategy == 'model_basic':\n",
    "        df = impute_model_basic(df)\n",
    "    elif impute_strategy == 'model_progressive':\n",
    "        df = impute_model_progressive(df)\n",
    "    else:\n",
    "        arr = SimpleImputer(missing_values=np.nan,strategy=impute_strategy).fit(\n",
    "          df.values).transform(df.values)\n",
    "        df = pd.DataFrame(data=arr, index=df.index.values, columns=df.columns.values)\n",
    "    if cols_to_standardize != None:\n",
    "        cols_to_standardize = list(set(cols_to_standardize) & set(df.columns.values))\n",
    "        df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "        df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "          df[cols_to_standardize]).transform(df[cols_to_standardize]),\n",
    "                                             index=df[cols_to_standardize].index.values,\n",
    "                                             columns=df[cols_to_standardize].columns.values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acoustic-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_attributes] = impute_data(df[numerical_attributes], 'model_progressive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-monitoring",
   "metadata": {},
   "source": [
    "#### 2.2 Imputation for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "boring-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na_freq(df, variable):\n",
    "    # find out most frequent category\n",
    "    most_frequent_category = df.groupby([variable])[variable].count().sort_values(ascending=False).index[0] \n",
    "    \n",
    "    ## replace missing values with most frequent category\n",
    "    df[variable].fillna(most_frequent_category, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "differential-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na_addCat(df, variable):\n",
    "    if((df[variable].isnull().sum())>0):\n",
    "        df[variable+'_NA'] = np.where(df[variable].isnull(), 'Missing', df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "temporal-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in categorical_attributes:\n",
    "    impute_na_addCat(df,cols)\n",
    "    impute_na_freq(df,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-mercy",
   "metadata": {},
   "source": [
    "### 3. Encoding of categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "guilty-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalEncoding_OneHot(df,variable):\n",
    "    return pd.get_dummies(df, columns=[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "annoying-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using weight of evidence encoding technique\n",
    "def CategoricalEncoding_WOE(df,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = df.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df[target_variable] == 0, target_variable] = 0.001\n",
    "    prob_df['WoE'] = np.log(prob_df[target_variable]/prob_df['target_0'])\n",
    "    ordered_labels = prob_df['WoE'].to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "convinced-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalEncoding_RiskFactor(df,variable,target_variable):\n",
    "    ordered_labels = df.groupby([variable])[target_variable].mean().to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "satellite-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalEncoding_Monotonicity(df,variable,target_variable):\n",
    "    ordered_labels=df.groupby([variable])[target_variable].mean().sort_values().index\n",
    "    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 1)}\n",
    "    df[variable+'_ordered']=df[variable].map(ordinal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "smooth-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalEncoding_PRE(df,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = df.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df['target_0'] == 0, 'target_0'] = 0.001\n",
    "    prob_df['PRE'] = prob_df[target_variable]/prob_df['target_0']\n",
    "    ordered_labels = prob_df['PRE'].to_dict()\n",
    "    df[variable+'_ordered'] = df[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-walker",
   "metadata": {},
   "source": [
    "Sample dataset categorical attributes is already encoded, hence no need for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "talented-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Family', 'Education']:\n",
    "#      df = CategoricalEncoding_OneHot(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-spirituality",
   "metadata": {},
   "source": [
    "### 4. Scaling of Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "irish-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Standard Scalar: z = (x - x_mean) / std\n",
    "def scaler_Standard(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for standardisation\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "relevant-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Mean Normalisation: z=(x-x_mean)/(x_max-x_min)\n",
    "def scaler_MeanNormalisation(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    means = df_x.mean(axis=0)\n",
    "    ranges = df_x.max(axis=0)-df_x.min(axis=0)\n",
    "    df_scaled = (df_x - means) / ranges\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adopted-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.MinMaxScaling:x_scaled=(x-x_min)/(x_max-x_min)\n",
    "def scaler_MinMax(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alert-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.MaxAbsScaling:x_scaled=x/x_max\n",
    "def scaler_MaxAbs(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    # set up the scaler\n",
    "    scaler = MaxAbsScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bigger-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.RobustScaling:x_scaled = x - x_median / ( x.quantile(0.75) - x.quantile(0.25) )\n",
    "def scaler_Robust(df):\n",
    "    # separate x and y\n",
    "    df_x = df.drop(columns=[target])\n",
    "    columns = df_x.columns\n",
    "    index = df_x.index\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    # set up the scaler\n",
    "    scaler = RobustScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(df_x)\n",
    "    # transform train and test sets\n",
    "    df_scaled = scaler.transform(df_x)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=index)\n",
    "    \n",
    "    # join back \n",
    "    df_scaled[target] = df[target]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "respective-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = scaler_Robust(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-poverty",
   "metadata": {},
   "source": [
    "### Save transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "genuine-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "      <th>Personal Loan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.254237</td>\n",
       "      <td>-0.863923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.508475</td>\n",
       "      <td>-1.241379</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.898305</td>\n",
       "      <td>0.475714</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.250278</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.322034</td>\n",
       "      <td>-0.781238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Experience    Income  ZIP Code  Family     CCAvg  Education  \\\n",
       "ID                                                                     \n",
       "1  -1.0       -0.95 -0.254237 -0.863923     1.0  0.055556       -0.5   \n",
       "2   0.0       -0.05 -0.508475 -1.241379     0.5  0.000000       -0.5   \n",
       "3  -0.3       -0.25 -0.898305  0.475714    -0.5 -0.277778       -0.5   \n",
       "4  -0.5       -0.55  0.610169  0.250278    -0.5  0.666667        0.0   \n",
       "5  -0.5       -0.60 -0.322034 -0.781238     1.0 -0.277778        0.0   \n",
       "\n",
       "    Mortgage  Securities Account  CD Account  Online  CreditCard  \\\n",
       "ID                                                                 \n",
       "1        0.0                 1.0         0.0    -1.0         0.0   \n",
       "2        0.0                 1.0         0.0    -1.0         0.0   \n",
       "3        0.0                 0.0         0.0    -1.0         0.0   \n",
       "4        0.0                 0.0         0.0    -1.0         0.0   \n",
       "5        0.0                 0.0         0.0    -1.0         1.0   \n",
       "\n",
       "    Personal Loan  \n",
       "ID                 \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "5               0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "composite-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.to_excel('../dataset/Bank_Personal_Loan_Modelling_transformed.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
