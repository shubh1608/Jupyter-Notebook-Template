{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessible-photography",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Notebook\n",
    "* we will use Scikit-Optimize, refere docs [here](https://scikit-optimize.github.io/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-publicity",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "minimal-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset info\n",
    "dataset_name = 'Bank_Personal_Loan_Modelling_transformed.xlsx'\n",
    "dataset_path = '../dataset/' + dataset_name\n",
    "dataset_format = 'xlsx'\n",
    "\n",
    "# target column for dataset\n",
    "target_col = 'Personal Loan'\n",
    "\n",
    "# where to save the model?\n",
    "model_store_location = '../store/model/'\n",
    "\n",
    "# model names\n",
    "model_names = ['LGBMClassifier', 'XGBClassifier', 'GradientBoostingClassifier']\n",
    "\n",
    "## define parameter search grid for all selected models here\n",
    "## do note that we need to define param grid as dict in case of grid search\n",
    "## uncomment other parameters before actual run\n",
    "#param_grid_search = {\n",
    "#     'n_estimators': range(20, 400, 40),\n",
    "#     'max_depth': range (1, 5, 1),\n",
    "#     'min_child_weight': range(1, 4, 1),\n",
    "#     'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#     'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,1],\n",
    "#     'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5]  \n",
    "#}\n",
    "params_grid_list = {\n",
    "    'LGBMClassifier': [\n",
    "        Integer(20, 400, name='n_estimators'),\n",
    "        Integer(1, 5, name='max_depth')\n",
    "    ],\n",
    "    'XGBClassifier': [\n",
    "        Integer(20, 400, name='n_estimators'),\n",
    "        Integer(1, 5, name='max_depth')\n",
    "    ],\n",
    "    'GradientBoostingClassifier': [\n",
    "        Integer(20, 400, name='n_estimators'),\n",
    "        Integer(1, 5, name='max_depth')\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# which cv technique you want to use?\n",
    "    # 'kfold'\n",
    "    # 'repeatedkfold'\n",
    "    # 'leaveoneout'\n",
    "    # 'leavepout'\n",
    "    # 'stratifiedkfold'\n",
    "    # 'groupkfold'\n",
    "    # 'grouplogo'\n",
    "    # 'timeserieskfold'\n",
    "\n",
    "# choose from above\n",
    "cv_technique = 'kfold'\n",
    "# which metric to optimize?\n",
    "metric = 'roc_auc'\n",
    "# maximize or minimize metric?\n",
    "maximize_metric = True\n",
    "\n",
    "# choose one search technique from below\n",
    "    # grid_search\n",
    "    # random_search\n",
    "    # bayesian_gp\n",
    "    # bayesian_forest_gp\n",
    "    # bayesian_gbrt_gp\n",
    "\n",
    "# todo: as of now we cant use the grid search as it expects params grid in dictionary\n",
    "# where as scikit optimize expects it in list, hence the grid above is in list form.\n",
    "# need to handle this internally so that we can define the params grid consistently\n",
    "search_technique = 'random_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-adams",
   "metadata": {},
   "source": [
    "### Steps for Hyperparameter Optimization\n",
    "1. Load pycaret's top 3 model saved in modelling notebook\n",
    "2. Define Hyperparameter Space\n",
    "3. Choose performance metrics to mimimize/maximize\n",
    "4. Choose Cross Validation Technique:\n",
    "    1. K-Fold Cross Validation\n",
    "    2. Leave one out\n",
    "    3. Leave p-out\n",
    "    4. Repeated K-Fold Cross Validation\n",
    "    5. Stratified K-Fold Cross Validation\n",
    "    6. Group Cross Validation\n",
    "    7. Nested Cross Validation\n",
    "5. Choose hyperparameter search process:\n",
    "    1. Manual approach\n",
    "    2. Automated approach\n",
    "       * Parallel \n",
    "        1. Grid Search \n",
    "        2. Random Search\n",
    "       * Sequential\n",
    "        1. Bayesin Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-junior",
   "metadata": {},
   "source": [
    "### INSTALL REQUIRED DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "angry-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hyperopt==0.2.5\n",
    "# !pip install scikit-optimize==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-promise",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cathedral-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    LeaveOneOut,\n",
    "    LeavePOut,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    LeaveOneGroupOut,\n",
    "    TimeSeriesSplit,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "# skopt search function\n",
    "from skopt import (\n",
    "    dummy_minimize, # for the randomized search\n",
    "    gp_minimize,\n",
    "    forest_minimize,\n",
    "    gbrt_minimize\n",
    ")\n",
    "\n",
    "# for the analysis\n",
    "from skopt.plots import (\n",
    "    plot_convergence,\n",
    "    plot_evaluations,\n",
    ")\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "severe-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to-do: note that we should be here with less features, target around 15-20 features\n",
    "## to-do: create research diagram for classification, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-kidney",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bacterial-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kfold_cv(model, x_train, y_train, n_splits = 5, metric = 'accuracy'):\n",
    "    # K-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=4)\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=kf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "alone-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_repeatedkfold_cv(model, x_train, y_train, n_splits = 5, n_repeats=10,  metric = 'accuracy'):\n",
    "    # Repeated K-Fold Cross-Validation\n",
    "    rkf = RepeatedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=4,\n",
    "    )\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=rkf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "hollow-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_leaveoneout_cv(model, x_train, y_train, metric = 'accuracy'):\n",
    "    # Leave One Out Cross-Validation\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=loo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "useful-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_leavepout_cv(model, x_train, y_train, p=2,  metric = 'accuracy'):\n",
    "    # Leave P Out Cross-Validation\n",
    "    lpo = LeavePOut(p=p)\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=lpo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "tested-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_stratifiedkfold_cv(model, x_train, y_train, n_splits = 5, n_repeats=10,  metric = 'accuracy'):\n",
    "    # stratified kfold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4)\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=skf \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-private",
   "metadata": {},
   "source": [
    "#### Group cross validation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "civic-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_group_kfold_cv(model, x_train, y_train, group_col, n_splits = 5, metric = 'accuracy'):\n",
    "    # Group K-Fold Cross-Validation\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=gkf.split(x_train.drop(group_col, axis=1), y_train, groups=x_train[group_col]) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "determined-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_group_logo_cv(model, x_train, y_train, group_col, metric = 'accuracy'):\n",
    "    # Cross-Validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=logo.split(x_train.drop(group_col, axis=1), y_train, groups=x_train[group_col]) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-respect",
   "metadata": {},
   "source": [
    "#### Time Series shuffle split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "distributed-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_timeseries_kfold_cv(model, x_train, y_train, n_splits=5, gap=0, metric = 'accuracy'):\n",
    "    # Cross-Validation\n",
    "    ts = TimeSeriesSplit(gap=gap, n_splits=n_splits)\n",
    "\n",
    "    # estimate generalization error\n",
    "    return  cross_validate(\n",
    "        model,\n",
    "        x_train, \n",
    "        y_train,\n",
    "        scoring=metric,\n",
    "        return_train_score=True,\n",
    "        cv=ts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-local",
   "metadata": {},
   "source": [
    "#### Create a dispatcher for cross validation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "illegal-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dispatcher = {\n",
    "    'kfold': perform_kfold_cv,\n",
    "    'repeatedkfold' : perform_repeatedkfold_cv,\n",
    "    'leaveoneout': perform_leaveoneout_cv,\n",
    "    'leavepout': perform_leavepout_cv,\n",
    "    'stratifiedkfold': perform_stratifiedkfold_cv,\n",
    "    'groupkfold': perform_group_kfold_cv,\n",
    "    'grouplogo': perform_group_logo_cv,\n",
    "    'timeserieskfold': perform_timeseries_kfold_cv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "chinese-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment below code to see the list of supported metrics in sklearn\n",
    "# import sklearn\n",
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "about-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_using_cv(models, x_train, y_train, metric, cv_technique):\n",
    "    for model_name in models.keys():\n",
    "        score = cv_dispatcher[cv_technique](models[model_name], x_train, y_train, metric=metric)\n",
    "        print(\"{} evaluation completed!\".format(model_name))\n",
    "        print('mean train set accuracy: ', np.mean(score['train_score']), ' +- ', np.std(score['train_score']))\n",
    "        print('mean test set accuracy: ', np.mean(score['test_score']), ' +- ', np.std(score['test_score']))\n",
    "        print('*'*100, end='\\n\\n')       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-standing",
   "metadata": {},
   "source": [
    "## Search Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "macro-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gridsearch(model,x_train, y_train, \n",
    "                          param_grid = param_grid_search, metric=metric, cv_technique = 'kfold'):\n",
    "    # to-do: add capability to take all cv techniques\n",
    "    # as of now gridsearch will support only kfold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "    \n",
    "    # grid search, i.e search all combinations\n",
    "    clf =  GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        scoring=metric,\n",
    "        cv=kf, # k-fold\n",
    "        refit=True, # refits best model to entire dataset\n",
    "    )\n",
    "\n",
    "    search = clf.fit(x_train, y_train)\n",
    "    results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "    best_params = search.best_params_\n",
    "    print(best_params)\n",
    "    return (search, best_params, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "guilty-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_randomsearch(objective, param_grid, n_calls=50):\n",
    "    # dummy_minimize performs the randomized search\n",
    "    search = dummy_minimize(\n",
    "        objective,  # the objective function to minimize\n",
    "        param_grid,  # the hyperparameter space\n",
    "        n_calls=n_calls,  # the number of evaluations of the objective function\n",
    "        random_state=0,\n",
    "    )\n",
    "    \n",
    "    res = {}\n",
    "    res['best_score'] = search.fun\n",
    "    best_params = {}\n",
    "    for idx, hyper_param in enumerate(param_grid):\n",
    "        best_params[hyper_param.name] = search.x[idx]\n",
    "    res['best_params'] = best_params\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "vanilla-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_gp(objective, param_grid, n_calls=30):\n",
    "    # dummy_minimize performs the randomized search\n",
    "    search = gp_minimize(\n",
    "        objective,  # the objective function to minimize\n",
    "        param_grid,  # the hyperparameter space\n",
    "        n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "        acq_func='EI', # the acquisition function\n",
    "        n_calls=n_calls,  # the number of evaluations of the objective function\n",
    "        random_state=0,\n",
    "    )\n",
    "    \n",
    "    res = {}\n",
    "    res['best_score'] = search.fun\n",
    "    best_params = {}\n",
    "    for idx, hyper_param in enumerate(param_grid):\n",
    "        best_params[hyper_param.name] = search.x[idx]\n",
    "    res['best_params'] = best_params\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "color-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_forestgp(objective, param_grid, n_calls=50):\n",
    "    # dummy_minimize performs the randomized search\n",
    "    search = forest_minimize(\n",
    "        objective,  # the objective function to minimize\n",
    "        param_grid,  # the hyperparameter space\n",
    "        base_estimator = 'RF', # the surrogate\n",
    "        n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "        acq_func='EI', # the acquisition function\n",
    "        n_calls=n_calls,  # the number of evaluations of the objective function\n",
    "        random_state=0,\n",
    "        n_jobs=4 # should be equal to number of cores for parallelization\n",
    "    )\n",
    "    \n",
    "    res = {}\n",
    "    res['best_score'] = search.fun\n",
    "    best_params = {}\n",
    "    for idx, hyper_param in enumerate(param_grid):\n",
    "        best_params[hyper_param.name] = search.x[idx]\n",
    "    res['best_params'] = best_params\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "editorial-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_gbrtgp(objective, param_grid, n_calls=50):\n",
    "    # dummy_minimize performs the randomized search\n",
    "    search = gbrt_minimize(\n",
    "        objective,  # the objective function to minimize\n",
    "        param_grid,  # the hyperparameter space\n",
    "        n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "        acq_func='EI', # the acquisition function\n",
    "        n_calls=n_calls,  # the number of evaluations of the objective function\n",
    "        random_state=0,\n",
    "        n_jobs=4 # should be equal to number of cores for parallelization\n",
    "    )\n",
    "    \n",
    "    res = {}\n",
    "    res['best_score'] = search.fun\n",
    "    best_params = {}\n",
    "    for idx, hyper_param in enumerate(param_grid):\n",
    "        best_params[hyper_param.name] = search.x[idx]\n",
    "    res['best_params'] = best_params\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "atmospheric-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_method_dispatcher = {\n",
    "    'grid_search': perform_gridsearch,\n",
    "    'random_search': perform_randomsearch,\n",
    "    'bayesian_gp': perform_bayesian_gp,\n",
    "    'bayesian_forestgp': perform_bayesian_forestgp,\n",
    "    'bayesian_gbrtgp': perform_bayesian_gbrtgp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "upper-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(model, x_train, y_train, x_test, y_test, param_grid, cv_technique, search_technique, metric, maximize_metric):\n",
    "    # objective function, takes param_grid and returns optimiztion score\n",
    "    @use_named_args(param_grid)\n",
    "    def objective(**params):\n",
    "        # model with new parameters\n",
    "        model.set_params(**params)\n",
    "        # tech debt\n",
    "        score = cv_dispatcher[cv_technique](model, x_train, y_train, metric=metric)\n",
    "        value = np.mean(score['test_score'])\n",
    "        \n",
    "        # todo: review when do we need to negate as per scikit-optimize\n",
    "        if (maximize_metric):\n",
    "            # negate because we need to maximize and scikit-optimize by default minimize\n",
    "            return -value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    res={}\n",
    "    if search_technique == 'grid_search':\n",
    "        print('search technique: grid_search, cv technique: {0}'.format(cv_technique))\n",
    "        res = perform_gridsearch(params_grid, x_train, y_train, metric)\n",
    "    else:\n",
    "        print('search technique: {0}, cv technique: {1}'.format(search_technique, cv_technique))\n",
    "        res = search_method_dispatcher[search_technique](objective, param_grid)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "personalized-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_models(dataset_path, dataset_format, target_col, test_size, model_store_location, model_names, params_grid_list, \n",
    "               cv_technique, search_technique, metric, maximize_metric):\n",
    "    # load dataset\n",
    "    df = None\n",
    "    if dataset_format == 'csv':\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    else:\n",
    "        df = pd.read_excel(dataset_path, index_col=0)\n",
    "    print('Dataset loaded!, size:({0}, {1})'.format(str(df.shape[0]), str(df.shape[1])), end='\\n\\n')\n",
    "        \n",
    "    # train - test split\n",
    "    # specify high test size for faster experimentation runs\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        df.drop(columns=[target_col]), \n",
    "        df[target_col], \n",
    "        test_size=test_size, \n",
    "        random_state=0\n",
    "    )\n",
    "    print('Split done!, train set size:({0}, {1})'.format(str(x_train.shape[0]), str(x_train.shape[1])), end='\\n\\n')\n",
    "    \n",
    "    # loading models from model store\n",
    "    models = {}\n",
    "    for name in model_names:\n",
    "        pipeline_and_model = load_model(model_store_location+name)\n",
    "        # pycaret seems to store this in last index, review this later - todo\n",
    "        model = pipeline_and_model[len(pipeline_and_model)-1]\n",
    "        models[name] = model\n",
    "        print('{0} loaded!'.format(name))\n",
    "    print('all models loaded!', end='\\n\\n')\n",
    "          \n",
    "    # run hyperparmeter seach\n",
    "    tuned_models={}\n",
    "    for model_name in models.keys():\n",
    "        model = models[model_name]\n",
    "        param_grid = params_grid_list[model_name]\n",
    "        print('hyperparmeter search started for {0}!'.format(model_name), end=' ... ')\n",
    "        res = run_search(model, x_train, y_train, x_test, y_test, param_grid, cv_technique, \n",
    "                                             search_technique, metric, maximize_metric)\n",
    "        print('search finished!')\n",
    "        print('best hyperparmeters:{0}'.format(res['best_params']))\n",
    "        print('best score:{0}'.format(res['best_score']), end='\\n\\n')\n",
    "        # tune model using best params\n",
    "        # tuned_model = tune_model(best_params)\n",
    "        # tuned_models[model_name] = tuned_model\n",
    "    \n",
    "    return tuned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "protected-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!, size:(5000, 13)\n",
      "\n",
      "Split done!, train set size:(500, 12)\n",
      "\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "LGBMClassifier loaded!\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "XGBClassifier loaded!\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "GradientBoostingClassifier loaded!\n",
      "all models loaded!\n",
      "\n",
      "hyperparmeter search started for LGBMClassifier! ... search technique: random_search, cv technique: kfold\n",
      "search finished!\n",
      "best hyperparmeters:{'n_estimators': 51, 'max_depth': 3}\n",
      "best score:-0.9925402854946739\n",
      "hyperparmeter search started for XGBClassifier! ... search technique: random_search, cv technique: kfold\n",
      "search finished!\n",
      "best hyperparmeters:{'n_estimators': 357, 'max_depth': 2}\n",
      "best score:-0.9899928835125035\n",
      "hyperparmeter search started for GradientBoostingClassifier! ... search technique: random_search, cv technique: kfold\n",
      "search finished!\n",
      "best hyperparmeters:{'n_estimators': 285, 'max_depth': 2}\n",
      "best score:-0.9929363932308171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_models(dataset_path, dataset_format, target_col, 0.9, model_store_location, model_names, params_grid_list, \n",
    "               cv_technique, search_technique, metric, maximize_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-government",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
