{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "logical-former",
   "metadata": {},
   "source": [
    "# A notebook template for quick prototyping of the solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-swing",
   "metadata": {},
   "source": [
    "### <u>TO-DO</u>\n",
    "* check for numerical attrib before using df.describe() and then list out numerical and categorical columns in Section 4.\n",
    "* we can have one section for handling class imbalances. Like code for SMOTE and other techniques.\n",
    "* one section for dimensionality reduction using PCA and other techniques.\n",
    "* use of df.nunique() for seeing the unique values per attribute, it makes it easy to spot the categorical and numerical attributes.\n",
    "* use a missing_value_table(df) method used in Home Credit analysis notebook, try considering it to have it in a library along with a common notebook.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-desktop",
   "metadata": {},
   "source": [
    "## 1. Setting workspace and environments\n",
    "* Use Anaconda\n",
    "* create a new virtual environment using conda\n",
    "* download the required libraries, for eg. using pip - \"pip install -U juyter matplotlib numpy pandas scipy scikit-learn\"\n",
    "* check installation by trying the import like - \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-blogger",
   "metadata": {},
   "source": [
    "## 2. Import required libraries\n",
    "* keep on adding as and when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "immune-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# classification models below\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-princess",
   "metadata": {},
   "source": [
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "native-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your dataset, can be a csv file or xlsx\n",
    "dataset_path = \"replace_with_path_to_your_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superb-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use code as per the type of data source\n",
    "\n",
    "## use below line to read data from csv file\n",
    "## df = pd.read_csv(dataset_path)\n",
    "\n",
    "# df = pd.read_excel(dataset_path, sheet_name = 1, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-nylon",
   "metadata": {},
   "source": [
    "## 4. Take a quick look at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "otherwise-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # good for getting a feel of the data\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "roman-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can help in spotting the presence of null values\n",
    "# # also can be used to see the column types\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opponent-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # value counts method can be used to see if an attribute contains categorical data or continous data\n",
    "# df['some_cateogorical_column'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-singapore",
   "metadata": {},
   "source": [
    "### List categorical attributes below\n",
    "* cat_attrib_1\n",
    "* cat_attrib_2\n",
    "* cat_attrib_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nearby-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare a list of categorical attributes\n",
    "# cat_attributes = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "transparent-reality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # use df.describe() for numerical column distribution\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-planet",
   "metadata": {},
   "source": [
    "### List numerical attributes below\n",
    "* num_attrib_1\n",
    "* num_attrib_2\n",
    "* num_attrib_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abstract-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare a list of numerical attributes\n",
    "# numerical_attributes = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # another good method to quickly see the column distribution is to use histogram plot.\n",
    "# # if attributes are more then try considering plotting histogram of subset of attributes.\n",
    "# df.hist(bins=50, figsize=(20, 15))\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-recorder",
   "metadata": {},
   "source": [
    "### For more detailed insights, use pandas profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expanded-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "velvet-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use - profile.to_notebook_iframe(), for rendering the report in an notebook iframe\n",
    "# profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-division",
   "metadata": {},
   "source": [
    "## 5. Create a Test set\n",
    "* Humans brain is an amazing pattern detection system, so its necessary to keep a test set separate before exploring it more, seeing the patterns in the test set may influence the model selection process. Read more about **data snooping bias**\n",
    "* read more about **stratified sampling**\n",
    "* *TO-DO: scope of adding different ways of creating the test set, refer HANDS ON ML: page 52*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clean-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using sklearn train_test_split method\n",
    "# # test_size: ratio of test set\n",
    "# # random_state: used for generating the same test set split every time\n",
    "# train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-brick",
   "metadata": {},
   "source": [
    "## 6. Discover and Visualize the Data to gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrapped-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets first create a copy so that we can play with it without harming the training set\n",
    "# train_copy = train_set.copy()\n",
    "\n",
    "# train_copy_num = train_copy[numerical_attributes]\n",
    "# train_copy_cat = train_copy[cat_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liked-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for numerical column comparison\n",
    "# it can help in seeing the correlation between dataset columns\n",
    "\n",
    "# # histogram/bar plots for seeing the categorical column distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appointed-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pearson's correlations between variables can be computed \n",
    "# # it can help in identifying which variables are correlated\n",
    "# # do note that the below method will only identify the linear relationships\n",
    "# corr_matrix = train_copy.corr()\n",
    "\n",
    "# # check for each variable using below code\n",
    "# corr_matrix[\"target variable\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "impaired-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # another way to identify the correlations is the use of pandas scatter_matrix function\n",
    "# # if there are high number of columns, then consider choosing a subset of those for which you want to see the corelations\n",
    "# # note that the histogram is plotted for a same pair of columns, as scatter plot wont make sense in this case\n",
    "\n",
    "# columns_of_interest = [\"Personal Loan\", \"Income\", \"Education\", \"Mortgage\", \"CCAvg\", \"Age\"]\n",
    "# scatter_matrix(train_copy[columns_of_interest], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-elimination",
   "metadata": {},
   "source": [
    "### Visualisation of continous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_hist(columns):\n",
    "    plt.subplots(2,2, figsize=(20,10))\n",
    "    plt.subplot(221)\n",
    "    plt.xlabel(columns[0])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.hist(customer_data[columns[0]], edgecolor = 'black')\n",
    "    if len(columns)>1:\n",
    "        plt.subplot(222)\n",
    "        plt.xlabel(columns[1])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.hist(customer_data[columns[1]], edgecolor = 'black')\n",
    "    if len(columns)>2:\n",
    "        plt.subplot(223)\n",
    "        plt.xlabel(columns[2])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.hist(customer_data[columns[2]], edgecolor = 'black')\n",
    "    if len(columns)>3:\n",
    "        plt.subplot(224)\n",
    "        plt.xlabel(columns[3])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.hist(customer_data[columns[3]], edgecolor = 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_dist(columns):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(20,10)) # Divide the plot into 2 rows, 2 columns\n",
    "    # Draw the plot in first row second column\n",
    "    sns.distplot( ax=axes[0,0],x=customer_data[columns[0]],hist=True)\n",
    "    axes[0,0].set_title(columns[0])\n",
    "    if len(columns)>1:\n",
    "        sns.distplot( ax=axes[0,1],x=customer_data[columns[1]],hist=True)\n",
    "        axes[0,1].set_title(columns[1])\n",
    "    if len(columns)>2:\n",
    "        sns.distplot( ax=axes[1,0],x=customer_data[columns[2]],hist=True)\n",
    "        axes[1,0].set_title(columns[2])\n",
    "    if len(columns)>3:\n",
    "        sns.distplot( ax=axes[1,1],x=customer_data[columns[3]],hist=True)\n",
    "        axes[1,1].set_title(columns[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plots(func,cols):\n",
    "    temp=0\n",
    "    length=len(cols)\n",
    "    if (length%4 == 0):\n",
    "        condition=length+1\n",
    "    else:\n",
    "        condition=(length+(4-(length%4)))+1\n",
    "    for i in range(4,condition,4):\n",
    "        if i<length:\n",
    "            func(cols[temp:i])\n",
    "        else:\n",
    "            func(cols[temp:length])\n",
    "        temp=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-syndicate",
   "metadata": {},
   "source": [
    "#### Bi variate analysis\n",
    "* If the target variable is categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-mauritius",
   "metadata": {},
   "source": [
    "#### Step1 - Analysis of continous variablle wrt categorical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_stripplot(columns,target):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(20,10)) # Divide the plot into 2 rows, 2 columns\n",
    "    # Draw the plot in first row second column\n",
    "    sns.stripplot(ax=axes[0,0],x=target , y = columns[0], data = customer_data)\n",
    "    axes[0,0].set_title(columns[0])\n",
    "    if len(columns)>1:\n",
    "        sns.stripplot( ax=axes[0,1],x=target , y = columns[1], data = customer_data)\n",
    "        axes[0,1].set_title(columns[1])\n",
    "    if len(columns)>2:\n",
    "        sns.stripplot( ax=axes[1,0],x=target , y = columns[2], data = customer_data)\n",
    "        axes[1,0].set_title(columns[2])\n",
    "    if len(columns)>3:\n",
    "        sns.stripplot( ax=axes[1,1],x=target , y = columns[3], data = customer_data)\n",
    "        axes[1,1].set_title(columns[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plots_bivariate(func,cols,target):\n",
    "    temp=0\n",
    "    length=len(cols)\n",
    "    if (length%4 == 0):\n",
    "        condition=length+1\n",
    "    else:\n",
    "        condition=(length+(4-(length%4)))+1\n",
    "    for i in range(4,condition,4):\n",
    "        if i<length:\n",
    "            func(cols[temp:i],target)\n",
    "        else:\n",
    "            func(cols[temp:length],target)\n",
    "        temp=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-amino",
   "metadata": {},
   "source": [
    "#### Step2-Analysis of discretised continuous variables wrt categorical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_EqualWidth(customer_data,variable):\n",
    "    # now let's capture the lower and upper boundaries\n",
    "    min_value = int(np.floor(customer_data[variable].min()))\n",
    "    max_value = int(np.ceil(customer_data[variable].max()))\n",
    "    range_value = max_value - min_value\n",
    "    # let's round the bin width\n",
    "    inter_value = int(np.round(range_value/10))\n",
    "    intervals = [i for i in range(min_value, max_value+inter_value, inter_value)]\n",
    "    labels = ['Bin_'+str(i) for i in range(1,len(intervals))]\n",
    "    # create one column with labels\n",
    "    customer_data[variable+'_disc_label'] = pd.cut(x = customer_data[variable], bins=intervals, labels=labels, include_lowest=True)\n",
    "    # and one with bin boundaries\n",
    "    customer_data[variable+'_disc'] = pd.cut(x = customer_data[variable], bins=intervals, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_count(columns,target):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(20,15)) # Divide the plot into 2 rows, 2 columns\n",
    "    # Draw the plot in first row second column\n",
    "    axes[0,0].tick_params('x', labelrotation=90)\n",
    "    sns.countplot(ax=axes[0,0],x=columns[0], hue=target, data = customer_data)\n",
    "    if len(columns)>1:\n",
    "        axes[0,1].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[0,1],x = columns[1],hue=target, data = customer_data)\n",
    "    if len(columns)>2:\n",
    "        axes[1,0].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[1,0],x = columns[2],hue=target, data = customer_data)\n",
    "    if len(columns)>3:\n",
    "        axes[1,1].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[1,1],x = columns[3],hue=target, data = customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perc(colname,target):\n",
    "    df1=pd.DataFrame(customer_data.groupby([colname])[target])\n",
    "    df1['perc']=0.0\n",
    "    df1['target_total']=0\n",
    "    df1['target_yes']=0\n",
    "    df1['target_no']=0\n",
    "    for i in range(0,len(df1)):\n",
    "        df2=df1[1][i]\n",
    "        total=len(df2)\n",
    "        if(total==0):\n",
    "            total=1\n",
    "        df2=df2[df2=='Yes']\n",
    "        count_yes=len(df2)\n",
    "        df1['target_total'][i]=total\n",
    "        df1['target_yes'][i]=count_yes\n",
    "        df1['target_no'][i]=total-count_yes\n",
    "        df1['perc'][i]=(count_yes/total)*100\n",
    "    df1.rename(columns={0:colname},inplace=True)\n",
    "    df1=df1.iloc[:,[0,2,3,4,5]]\n",
    "    df1=df1.sort_values(by='perc',ascending=False)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-place",
   "metadata": {},
   "source": [
    "### Visualisation of categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_count(columns):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(20,15)) # Divide the plot into 2 rows, 2 columns\n",
    "    # Draw the plot in first row second column\n",
    "    axes[0,0].tick_params('x', labelrotation=90)\n",
    "    sns.countplot(ax=axes[0,0],x=columns[0],data = customer_data)\n",
    "    if len(columns)>1:\n",
    "        axes[0,1].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[0,1],x = columns[1], data = customer_data)\n",
    "    if len(columns)>2:\n",
    "        axes[1,0].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[1,0],x = columns[2], data = customer_data)\n",
    "    if len(columns)>3:\n",
    "        axes[1,1].tick_params('x', labelrotation=90)\n",
    "        sns.countplot( ax=axes[1,1],x = columns[3], data = customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-cheese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alike-method",
   "metadata": {},
   "source": [
    "## 6.2 Outlier detection and treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-retention",
   "metadata": {},
   "source": [
    "### Handling outliers for numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-projection",
   "metadata": {},
   "source": [
    "#### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.boxplot(column = ['pdays'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with zero coding-Any value less than zero will be made zero\n",
    "def outliers_ZeroCoding(X,variable):\n",
    "    X.loc[X[variable]<0, variable] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-passport",
   "metadata": {},
   "source": [
    "### Outlier imputation-Non-normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_quantile(X_train,X_test,variable):\n",
    "    # top coding: upper boundary for outliers according to interquantile proximity rule\n",
    "    IQR = X_train[variable].quantile(0.75) - X_train[variable].quantile(0.25)\n",
    "    Upper_fence = X_train[variable].quantile(0.75) + (IQR * 3)\n",
    "    X_train.loc[X_train[variable]>Upper_fence, variable] = Upper_fence\n",
    "    X_test.loc[X_test[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_quantile(X_train,X_test,variable):\n",
    "    # bottom coding: lower boundary for outliers according to interquantile proximity rule\n",
    "    IQR = X_train[variable].quantile(0.75) - X_train[variable].quantile(0.25)\n",
    "    Lower_fence = X_train[variable].quantile(0.25) - (IQR * 3)\n",
    "    X_train.loc[X_train[variable]<Lower_fence, variable] = Lower_fence\n",
    "    X_test.loc[X_test[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in Numerical_columns:\n",
    "    outliers_TopCoding_quantile(X_train,X_test,cols)\n",
    "    outliers_BottomCoding_quantile(X_train,X_test,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-looking",
   "metadata": {},
   "source": [
    "### Outlier imputation-normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value greater than maximum limit will be capped at maximum\n",
    "def outliers_TopCoding_gaussian(X_train,X_test,variable):\n",
    "    # top coding: upper boundary for outliers according to gaussian rule\n",
    "    Upper_fence = X_train[variable].mean()+3*X_train[variable].std()\n",
    "    X_train.loc[X_train[variable]>Upper_fence, variable] = Upper_fence\n",
    "    X_test.loc[X_test[variable]>Upper_fence, variable] = Upper_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating outliers with top coding-Any value less than minimum limit will be capped at minimum\n",
    "def outliers_BottomCoding_gaussian(X_train,X_test,variable):\n",
    "    # bottom coding: lower boundary for outliers according to gaussian rule\n",
    "    Lower_fence = X_train[variable].mean()-3*X_train[variable].std()\n",
    "    X_train.loc[X_train[variable]<Lower_fence, variable] = Lower_fence\n",
    "    X_test.loc[X_test[variable]<Lower_fence, variable] = Lower_fence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-position",
   "metadata": {},
   "source": [
    "Next step will be to convert these non-normal distribution to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_BoxCox(X_train,X_test,variable):\n",
    "    X_train[variable+'_boxcox'], param = stats.boxcox(X_train[variable])\n",
    "    X_test[variable+'_boxcox'], param = stats.boxcox(X_test[variable])\n",
    "    print('Optimal lambda: ', param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in Numerical_columns:\n",
    "    transform_BoxCox(X_train,X_test,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-forge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "french-carnival",
   "metadata": {},
   "source": [
    "### Handling  outliers for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_new_imputation(X_train,X_test,variable,rare_cat):\n",
    "    temp = X_train.groupby([variable])[variable].count()/np.float(len(X_train))\n",
    "    rare_cat = [x for x in temp.loc[temp<0.05].index.values]\n",
    "    X_train[variable+'_rare_imp'] = np.where(X_train[variable].isin(rare_cat), 'Others', X_train[variable])\n",
    "    X_test[variable+'_rare_imp'] = np.where(X_test[variable].isin(rare_cat), 'Others', X_test[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_freq_imputation(X_train,X_test,variable,rare_cat,frequent_cat):\n",
    "    # create new variables, with freq labels imputed\n",
    "    \n",
    "    # by the most frequent category\n",
    "    X_train[variable+'_freq_imp'] = np.where(X_train[variable].isin(rare_cat), frequent_cat, X_train[variable])\n",
    "    X_test[variable+'_freq_imp'] = np.where(X_test[variable].isin(rare_cat), frequent_cat, X_test[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-cocktail",
   "metadata": {},
   "source": [
    "Group all categories under 5% into one common category 'others' or into the most frequent category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in Categorical_Columns:\n",
    "     # find the most frequent category\n",
    "    frequent_cat = X_train.groupby(cols)[cols].count().sort_values().tail(1).index.values[0]\n",
    "    \n",
    "    # find rare labels\n",
    "    temp = X_train.groupby([cols])[cols].count()/np.float(len(X_train))\n",
    "    rare_cat = [x for x in temp.loc[temp<0.05].index.values]\n",
    "    #Introduce a new label only if there are more than one rare categories.else combine the rare \n",
    "    #category with most frequent one\n",
    "    if len(rare_cat) > 1:\n",
    "        rare_new_imputation(X_train,X_test,cols,rare_cat)\n",
    "    else:\n",
    "        rare_freq_imputation(X_train,X_test,cols,rare_cat,frequent_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-qatar",
   "metadata": {},
   "source": [
    "### Descretization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-valuable",
   "metadata": {},
   "source": [
    "* Equal frequency dicretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_EqualFreq(X_train,X_test,variable):\n",
    "    # create 10 labels, one for each quantile\n",
    "    labels = ['Q'+str(i+1) for i in range(0,10)]\n",
    "    # bins with labels\n",
    "    # precision=3 means up to 3 places of decimal\n",
    "    #X_train[variable+'_disc_label'], bins = pd.qcut(x=X_train[variable], q=10, retbins=True, precision=3, duplicates='drop')\n",
    "    # bins with boundaries\n",
    "    X_train[variable+'_disc'], bins = pd.qcut(x=X_train[variable], q=10, retbins=True, precision=3, duplicates='drop')\n",
    "    #X_test[variable+'_disc_label'] = pd.cut(x = X_test.Age, bins=bins)\n",
    "    X_test[variable+'_disc'] = pd.cut(x = X_test.Age, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-dylan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "floral-lodge",
   "metadata": {},
   "source": [
    "* Equal width dicretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_EqualWidth(X_train,X_test,variable):\n",
    "    # now let's capture the lower and upper boundaries\n",
    "    min_value = int(np.floor(X_train[variable].min()))\n",
    "    max_value = int(np.ceil(X_train[variable].max()))\n",
    "    range_value = max_value - min_value\n",
    "    # let's round the bin width\n",
    "    inter_value = int(np.round(range_value/10))\n",
    "    intervals = [i for i in range(min_value, max_value+inter_value, inter_value)]\n",
    "    labels = ['Bin_'+str(i) for i in range(1,len(intervals))]\n",
    "    # create one column with labels\n",
    "    X_train[variable+'_disc_label'] = pd.cut(x = X_train[variable], bins=intervals, labels=labels, include_lowest=True)\n",
    "    # and one with bin boundaries\n",
    "    X_train[variable+'_disc'] = pd.cut(x = X_train[variable], bins=intervals, include_lowest=True)\n",
    "    X_test[variable+'_disc_label'] = pd.cut(x = X_test[variable], bins=intervals, labels=labels, include_lowest=True)\n",
    "    X_test[variable+'_disc'] = pd.cut(x = X_test[variable], bins=intervals, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use any of the above 2 methods the for loop of all numerical columns\n",
    "for cols in Numerical_columns:\n",
    "    disc_EqualFreq(X_train,X_test,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-vision",
   "metadata": {},
   "source": [
    "## 7. Experiment with attribute combinations\n",
    "* There can be cases where a particular attribute in itself wont add any value, but it can be combined with other attributes to see the affect of that combined variable on the predictor variable.\n",
    "* Example - Suppose the number of rooms in a district is not usesful if you dont know how many households are there in the distrcit. So in this case a new variable \"bedroom_per_household\" should be added in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "radical-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence study the attributes, understand them properly and figure out what attributes can be combined\n",
    "# create a new column in the dataset for the combined attributes\n",
    "# see again the correlation of attributes with the dependent/predictor variables\n",
    "# if its correlation is strong, then it can be used otherwise dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-commander",
   "metadata": {},
   "source": [
    "## 8. Prepare the data for machine learning algorithms\n",
    "\n",
    "<br/>instead of doing this manually it's important to write a functions which can be helpful in below scenarios\n",
    "* reproduce transformations later\n",
    "* helpful in buidling a library of these functiosn gradually\n",
    "* can use these functions in the live system\n",
    "* can be helpful in easily trying out various trasnformations and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "level-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It's a good idea at this point to separate predictors and the target values/labels from the train_set\n",
    "# # as we dont want to apply the same transofrmation on the traget values\n",
    "# train_label = train_copy[\"target_variable\"].copy()\n",
    "# train_copy = train_copy.drop(\"target_variable\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-sellers",
   "metadata": {},
   "source": [
    "## 9. Data Cleaning\n",
    "Most machine learning algorithms can not work with the missing values, hence it's important to take care of them and there are options below how you can handle them.\n",
    "1. get rid of rows/values where the value is null\n",
    "2. get rid of columns where significant percentage of value is null\n",
    "3. set the values to some value, like mean, the median and as per domain understanding may be in some case 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-jimmy",
   "metadata": {},
   "source": [
    "### Missing values Treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-paradise",
   "metadata": {},
   "source": [
    "### Handling numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "packed-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option 1: \n",
    "# train_copy.drop(subset=[\"columns which has null value\"])\n",
    "\n",
    "# # option 2: \n",
    "# train_copy.drop(\"column having null value\", axis=1)\n",
    "\n",
    "# # option 3: \n",
    "# median = train_copy[\"column having null value\"].median()\n",
    "# train_copy[\"column having null value\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-belgium",
   "metadata": {},
   "source": [
    "* do note that the median values and the other values which will be used for imputation should be saved/noted somewhere so that they can used later to replace missing values in the test dataset and even in the live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "handled-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scikit learn also provides a handly class: SimpleImputer to take care of the missing values\n",
    "\n",
    "# # below line is essentially saying that we need a imputer which can use a strategy(in this case \"median\") \n",
    "# # to replace the missing value in the dataset\n",
    "# # also make sure to copy only the numerical attribute in to a temporary dataframe as this step wont be applicable for text attr.\n",
    "\n",
    "# imputer = SimpleImputer(strategy = \"median\")\n",
    "\n",
    "# imputer.fit(train_copy_num)\n",
    "\n",
    "# X = imputer.transform(train_copy_num) # this will return an numpy array\n",
    "\n",
    "# train_copy_tr = pd.DataFrame(X, columns = train_copy_num.columns, index=train_copy_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors\n",
    "def impute_model_basic(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    for col in cols_nan:\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors,\n",
    "#   including progressively added imputed features\n",
    "def impute_model_progressive(df):\n",
    "    cols_nan = df.columns[df.isna().any()].tolist()\n",
    "    cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    while len(cols_nan)&amp;gt;0:\n",
    "        col = cols_nan[0]\n",
    "        test_data = df[df[col].isna()]\n",
    "        train_data = df.dropna()\n",
    "        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "        df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "        cols_nan = df.columns[df.isna().any()].tolist()\n",
    "        cols_no_nan = df.columns.difference(cols_nan).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for imputing missing data according to a given impute_strategy:\n",
    "#  drop_rows: drop all rows with one or more missing values\n",
    "#  drop_cols: drop columns with one or more missing values\n",
    "#  model_basic: KNN-model-based imputation with fixed predictors\n",
    "#  model_progressive: KNN-model-based imputation with progressively added predictors\n",
    "#  mean, median, most_frequent: imputation with mean, median or most frequent values\n",
    "#\n",
    "#  cols_to_standardize: if provided, the specified columns are scaled between 0 and 1, after imputation\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'drop_rows':\n",
    "        df = df.dropna(axis=0)\n",
    "    elif impute_strategy == 'drop_cols':\n",
    "        df = df.dropna(axis=1)\n",
    "    elif impute_strategy == 'model_basic':\n",
    "        df = impute_model_basic(df)\n",
    "    elif impute_strategy == 'model_progressive':\n",
    "        df = impute_model_progressive(df)\n",
    "    else:\n",
    "        arr = SimpleImputer(missing_values=np.nan,strategy=impute_strategy).fit(\n",
    "          df.values).transform(df.values)\n",
    "        df = pd.DataFrame(data=arr, index=df.index.values, columns=df.columns.values)\n",
    "    if cols_to_standardize != None:\n",
    "        cols_to_standardize = list(set(cols_to_standardize) &amp;amp; set(df.columns.values))\n",
    "        df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "        df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "          df[cols_to_standardize]).transform(df[cols_to_standardize]),\n",
    "                                             index=df[cols_to_standardize].index.values,\n",
    "                                             columns=df[cols_to_standardize].columns.values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-watson",
   "metadata": {},
   "source": [
    "### Handling Categorical and Text attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-suite",
   "metadata": {},
   "source": [
    "* text should be processed differently as per the requirement, and it should be clear when it is analysed\n",
    "* steps which would be common in all problems is to convert the categorical column in to numbers as ML algorithms can understand only numbers.\n",
    "* There are multiple approaches to achieve this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-retrieval",
   "metadata": {},
   "source": [
    "1. ORDINAL ENCODER - this will convert the categories in to numbers, like [\"bad\", \"average\", \"good\"] in to [0, 1, 2], this is good approach when there is order between the categories as this pattern will be learnt by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attached-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a copy of dataframe with categorical attributes\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# train_copy_cat_encoded = oridnal_encoder.fit_transform(train_copy_cat)\n",
    "# ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-theory",
   "metadata": {},
   "source": [
    "2. OneHotENCODER - This type of encoding is good when we dont want to preserver any order/relation between the categories. This will represent a sparse matrix which stores the condensed information. To see the full array use the toarray() method, the number of rows will be equal to the dataset size and columns will be equal to the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "settled-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_encoder = OneHotEncoder()\n",
    "# train_copy__cat_1hot = cat_encoder.fit_transform(train_copy_cat)\n",
    "# cat_encoder.categories_\n",
    "# train_copy__cat_1hot.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-while",
   "metadata": {},
   "source": [
    "NOTE: What if our categorical attribute has large number of categories, like country code, profession, species. Converting attribtues like this in to one hot encoding will lead to the drastic increase in the large number of columns which will increase the training time and degrade the performance. \n",
    "* one approach to handle this is to replace categories with the numbers which is representative of the category value such as country code can be replaced by country's population or GDP.\n",
    "* Alternatively, we can replace each category with a learnable, low-dimensional vector called an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-council",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using weight of evidence encoding technique\n",
    "def CategoricalImputation_WOE(X_train,X_test,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = X_train.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df[target_variable] == 0, target_variable] = 0.001\n",
    "    prob_df['WoE'] = np.log(prob_df[target_variable]/prob_df['target_0'])\n",
    "    ordered_labels = prob_df['WoE'].to_dict()\n",
    "    X_train[variable+'_ordered'] = X_train[variable].map(ordered_labels)\n",
    "    X_test[variable+'_ordered'] = X_test[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalImputation_RiskFactor(X_train,X_test,variable,target_variable):\n",
    "    ordered_labels = X_train.groupby([variable])[target_variable].mean().to_dict()\n",
    "    X_train[variable+'_ordered'] = X_train[variable].map(ordered_labels)\n",
    "    X_test[variable+'_ordered'] = X_test[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalImputation_Monotonicity(X_train,X_test,variable,target_variable):\n",
    "    ordered_labels=X_train.groupby([variable])[target_variable].mean().sort_values().index\n",
    "    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 1)}\n",
    "    X_train[variable+'_ordered']=X_train[variable].map(ordinal_label)\n",
    "    X_test[variable+'_ordered']=X_test[variable].map(ordinal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels by risk factor encoding technique\n",
    "def CategoricalImputation_PRE(X_train,X_test,variable,target_variable):\n",
    "    # now we calculate the probability of target=1 \n",
    "    prob_df = X_train.groupby([variable])[target_variable].mean()\n",
    "    prob_df = pd.DataFrame(prob_df)\n",
    "    \n",
    "    # and now the probability of target = 0 \n",
    "    # and we add it to the dataframe\n",
    "    prob_df['target_0'] = 1-prob_df[target_variable]\n",
    "    prob_df.loc[prob_df['target_0'] == 0, 'target_0'] = 0.001\n",
    "    prob_df['PRE'] = prob_df[target_variable]/prob_df['target_0']\n",
    "    ordered_labels = prob_df['PRE'].to_dict()\n",
    "    X_train[variable+'_ordered'] = X_train[variable].map(ordered_labels)\n",
    "    X_test[variable+'_ordered'] = X_test[variable].map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in Categorical_Columns:\n",
    "     CategoricalImputation_WOE(X_train,X_test,cols,'Claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-runner",
   "metadata": {},
   "source": [
    "### Custom Transformers\n",
    "\n",
    "There are a lot of transformers available in scikit learn, but often you will require to write your own custom transformers or custom cleanup operations or combining attributes.\n",
    "\n",
    "#### Why you want to do it?\n",
    "So that our custom transformer works seamlessly with the Scikit Learn functionalities such as pipelines\n",
    "\n",
    "#### How you can do it?\n",
    "Using duck typing since scikit learn relies on duck typing instead of inhertitance. So we need to create a class and implement three methods fit(), transform() and fit_transform()\n",
    "\n",
    "#### Other benefits?\n",
    "We will also get two extra methods get_params() and set_params() that will help in hyperparameter tuning. Adding hyperparameters in the custom class will easily led the automated process to find out the optimal value for this hyperparameter. More genrally we can add a hyperparameter for each step for which we are not sure about. Advantage is that the more combination we can try out the more likely it will be to find a great combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-attraction",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "It's one of the most important transformations on the data. With few exceptions, most of the algorithms dont perform well when the attributes are in different scale. \n",
    "\n",
    "There are two common ways to scale the attributes:\n",
    "1. Min-max scaling/Normalization - simple and values are shifted and scaled so that all the values comes in range of 0-1. This is achieved by subtracting the min value and then divide by the max - min.\n",
    "2. Standardization - First it subtracts the mean value so that the standardised values always has 0 mean. And then divide by the standard deviation so that resulting distribution has unit variance. \n",
    "\n",
    "Standardization does not reduce the inputs to a range of values like normalisation, this can be a problem for some of the algorithms like Neural Networks, which expects the value to be in a range of 0-1. However, Standardization is much less affected by the outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-living",
   "metadata": {},
   "source": [
    "1.Standard Scalar: z = (x - x_mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_Standard(X_train,X_test):\n",
    "    # the scaler - for standardisation\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    return X_train_scaled,X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-hours",
   "metadata": {},
   "source": [
    "2.Mean Normalisation: z=(x-x_mean)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caroline-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_MeanNormalisation(X_train,X_test):\n",
    "    means = X_train.mean(axis=0)\n",
    "    ranges = X_train.max(axis=0)-X_train.min(axis=0)\n",
    "    X_train_scaled = (X_train - means) / ranges\n",
    "    X_test_scaled = (X_test - means) / ranges\n",
    "    return X_train_scaled,X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-spice",
   "metadata": {},
   "source": [
    "3.MinMaxScaling:x_scaled=(x-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_MinMax(X_train,X_test):\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    return X_train_scaled,X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-generation",
   "metadata": {},
   "source": [
    "4.MaxAbsScaling:x_scaled=x/x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_MaxAbs(X_train,X_test):\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    # set up the scaler\n",
    "    scaler = MaxAbsScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    return X_train_scaled,X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-algeria",
   "metadata": {},
   "source": [
    "5.RobustScaling:X_scaled = X - X_median / ( X.quantile(0.75) - X.quantile(0.25) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_Robust(X_train,X_test):\n",
    "    # the scaler - for min-max scaling\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    # set up the scaler\n",
    "    scaler = RobustScaler()\n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # let's transform the returned NumPy arrays to dataframes \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    return X_train_scaled,X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-happening",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "Notice that the transformation steps can be applied sequentially on the dataset. Sklearn provides a pipeline class to apply the transformation methods in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "american-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be used to apply the transformations on the same set of columns\n",
    "# hence we need a separate step just for the numerical attributes\n",
    "\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy = \"median\")),   # missing value treatement\n",
    "#     ('std_scaler', StandardScaler())                   # standard scaling on numerical attributes\n",
    "#     ('attribs_adder', CombinedAttributesAdder()),      # custom transformer\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-accommodation",
   "metadata": {},
   "source": [
    "NOTE: till now we have applied the transformations on the numerical and categorical attribute separately. It would be more convenient to do it in a single step. fortunately, sklearn provides a class - **ColumnTransformer** for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acute-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_attributes = list(train_copy_num)\n",
    "# cat_attributes = list(train_copy_cat)\n",
    "\n",
    "# full_pipeline = ColumnTransformer([\n",
    "#     (\"num\", num_pipeline, num_attributes),\n",
    "#     (\"cat\", OneHotEncoder(), cat_attributes)\n",
    "# ])\n",
    "\n",
    "# train_copy_transformed = full_pipeline.fit_transform(train_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-companion",
   "metadata": {},
   "source": [
    "In above pipeline class, instead of mentioning the transformer we can specify \"drop\" or \"pass-through\" for columns to be dropped or columns to be left untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-swing",
   "metadata": {},
   "source": [
    "## 10. Select and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-central",
   "metadata": {},
   "source": [
    "### Shortlist classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "static-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = {\n",
    "#     \"Logistics Regression\": LogisticRegression(),\n",
    "#     \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "#     \"Linear SVM\": SVC(kernel=\"linear\", C=0.025),\n",
    "#     \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "#     \"Gaussian Process\": GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(max_depth=5),\n",
    "#     \"Random Forest\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     \"Neural Net\": MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     \"AdaBoost\": AdaBoostClassifier(),\n",
    "#     \"Naive Bayes\": GaussianNB(),\n",
    "#     \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "# }\n",
    "\n",
    "# model.fit(train_copy, train_label)\n",
    "\n",
    "# testing on train dataset \n",
    "# train_data_subset = train_copy.iloc[:5] # for example take 5\n",
    "# train_data_subset_labels = train_label.iloc[:5]\n",
    "\n",
    "# train_subset_prepared = fullpipeline.transform(train_copy_subset)\n",
    "# print(\"Predictions:\", model.predict(train_subset_prepared))\n",
    "# print(\"Labels:\", train_data_subset_labels)\n",
    "\n",
    "# # calculating the classification accuracy on whole train set\n",
    "# train_copy_predictions = model.predict(train_copy_prepared)\n",
    "# print(metrics.classification_report(train_labels, train_copy_predictions))\n",
    "# print(\"roc_auc_score: \", roc_auc_score(train_labels, train_copy_predictions))\n",
    "# print(\"f1 score: \", f1_score(train_labels, train_copy_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-sewing",
   "metadata": {},
   "source": [
    "### Shortlist regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "invisible-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "# model = DecisionTreeRegressor()\n",
    "# model = RandomForestRegressor()\n",
    "\n",
    "# model.fit(train_copy, train_label)\n",
    "\n",
    "# testing on train dataset \n",
    "# train_data_subset = train_copy.iloc[:5] # for example take 5\n",
    "# train_data_subset_labels = train_label.iloc[:5]\n",
    "\n",
    "# train_subset_prepared = fullpipeline.transform(train_copy_subset)\n",
    "# print(\"Predictions:\", model.predict(train_subset_prepared))\n",
    "# print(\"Labels:\", train_data_subset_labels)\n",
    "\n",
    "# # calculating the mean squared error on whole train set\n",
    "# train_copy_predictions = model.predict(train_copy_prepared)\n",
    "# model_mse = mean_squared_error(train_labels, train_copy_predictions)\n",
    "# model_rmse = np.sqrt(model_mse)\n",
    "# print(\"RMSE on train dataset:\", model_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-disclaimer",
   "metadata": {},
   "source": [
    "Sometimes you will notice that the model's score on the train dataset is ~ 100% or unusually high. This indicates that the model has overfit the data. To confirm this a model can be tested on the test dataset(unseen), but we can not do it as the model will see the test data in this case.\n",
    "\n",
    "To avoid this K-Fold cross validation strategy can be used where a dataset is divided into k folds and a model is trained on k-1 fold and tested on 1 fold with k number of iterations and each iterations test set being different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-canyon",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nearby-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here scoring parameter can be used to specify the custom scoring metrics type\n",
    "# # for regression we can use scoring = \"neg_mean_squared_error\"\n",
    "# # for classification we can use scoring = \"f1_macro\"\n",
    "# # by default it uses the estimator scoring parameter\n",
    "\n",
    "# scores = cross_val_score(model, train_copy_prepared, train_labels, \n",
    "#                          scoring = \"decide_scoring_metric\", cv=10)\n",
    "\n",
    "# # below is only for regression models when scoring = \"neg_mean_squared_error\"\n",
    "# scores = np.sqrt(-scores)\n",
    "\n",
    "# def display_scores(scores):\n",
    "#     print(\"Scores:\", scores)\n",
    "#     print(\"Mean:\", scores.mean())\n",
    "#     print(\"Standard Deviation\", scores.std())\n",
    "\n",
    "# display_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-skill",
   "metadata": {},
   "source": [
    "### Save model frequently\n",
    "* a good idea is to save model so that you do not need to retrain and model can be compared easily.\n",
    "* Joblib library can be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blank-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(model, \"model.pkl\")\n",
    "\n",
    "# ... and later\n",
    "# model_loaded = joblib.load(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-milan",
   "metadata": {},
   "source": [
    "## 11: Fine Tune your shortlisted models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-ireland",
   "metadata": {},
   "source": [
    "### 1.Grid Search\n",
    "There are a lot of hyperparameter values which you need to evaluated. This will be a tedious task if you have to run it every time with different hyperparameter values. Grid Search will help here by automating this process, essentially it will try out each and every combination(searching in grid/matrix) and will present a list of hyperparameter for which model performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "removable-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a params_grid like below\n",
    "# param_grid = [\n",
    "#     {'param_set_1' : [1, 5, 10 ...], 'param_set_2' : [0.5, 0.75. 1 ...]}\n",
    "# ]\n",
    "\n",
    "# # take your shortlisted model here, i.e you want to hypertune that model more\n",
    "# regressor_or_classifier = ShortListedModel()\n",
    "# grid_search = GridSearchCV(regressor_or_classifier, param_grid, cv=5, \n",
    "#                            scoring='decide_scoring_metric', return_train_score = True)\n",
    "# grid_search.fit(train_copy_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-debate",
   "metadata": {},
   "source": [
    "see your best params list using below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "integral-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-scholar",
   "metadata": {},
   "source": [
    "you can also get the best estimator directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "junior-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-colonial",
   "metadata": {},
   "source": [
    "evaluation scores are also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "willing-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_res = grid_search.cv_results_\n",
    "# for mean_score, params in zip(cv_res[\"mean_test_score\"], cv_res[\"params\"]):\n",
    "#     print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-hypothesis",
   "metadata": {},
   "source": [
    "*At this moment, you have successfully fine tuned your model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-ballet",
   "metadata": {},
   "source": [
    "NOTE: Do note that we can treat some of the data preapration steps also as a hyperparameter. For eg. having a combined/engineered attribute will help or not, to automatically find a best way of handling outliers, feature selection and more. The transformers can be developed accordingly and can be used as a hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-parcel",
   "metadata": {},
   "source": [
    "### 2.Randomized Search\n",
    "* Using Grid Search is fine when you have comparatively less combinations of hyperparmeter values to test. But what if we have a large number of combinations? in this case instead of evaluating each and every combination from a large search space, randomized search approach can be used which will evaluate the combinatiosn by taking the random values for a hyperparameter at every iterations.\n",
    "\n",
    "This approach has two main benefits\n",
    "1. if we let the randomized search to run for 1000 iterations, then it will explore 1000 different values for a hyperparamter instead of just few values of a hyperparameter with the grid search approach.\n",
    "2. Simply by setting the number of iterations, you have more control over the computing budget you want to allocate to hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-johnston",
   "metadata": {},
   "source": [
    "### 3. Ensemble methods\n",
    "This is one another way of fine tuning your models. Often a group of models will perform best as compared to just usin a single model such as Random Forest over Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-appraisal",
   "metadata": {},
   "source": [
    "### 4. Analyze the best models and their errors\n",
    "you will gain good insights when you inspect the models. For example RandomForest models can indicate the relative importance of each attribute for making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "difficult-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "# feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-fraud",
   "metadata": {},
   "source": [
    "Let's display these importance scores beside the attributes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "subtle-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes = numerical_attrib + cat_attrib + extra_attrib\n",
    "\n",
    "# # Do apply transformations on it based on the transformation pipeline you created before\n",
    "# sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-sunrise",
   "metadata": {},
   "source": [
    "With information from above, you can try inspecting the contribution from each attributes and you can decide to drop few of those which seems to contribtue less in the models prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-conviction",
   "metadata": {},
   "source": [
    "## 12. Evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "opponent-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = grid_search.best_estimator_\n",
    "\n",
    "# x_test = test_set.drop(\"target_variable\", axis=1)\n",
    "# y_test = test_set[\"target_variable\"].copy()\n",
    "\n",
    "# x_test_prepared = full_pipeline.transform(x_test)\n",
    "\n",
    "# final_predictions = final_model.predict(x_test_prepared)\n",
    "\n",
    "# # use below for classifiers\n",
    "# print(metrics.classification_report(train_labels, train_copy_predictions))\n",
    "# print(\"roc_auc_score: \", roc_auc_score(train_labels, train_copy_predictions))\n",
    "# print(\"f1 score: \", f1_score(train_labels, train_copy_predictions))\n",
    "\n",
    "# use below for regressors\n",
    "# final_mse = mean_squared_error(y_test, final_predictions)\n",
    "# final_rmse = np.sqrt(final_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
